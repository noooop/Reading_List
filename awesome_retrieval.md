
# Overview
å€Ÿ [ColBERT](https://arxiv.org/abs/2004.12832) çš„å›¾ï¼Œå¹¿ä¹‰çš„è®¡ç®—å¥å­ä¹‹é—´ç›¸ä¼¼åº¦çš„æ¨¡å‹å¤§æ¦‚æœ‰å››ç§æ¶æ„ï¼š

<img src="https://github.com/noooop/noooop.github.io/blob/main/applications/rag/colbert.png?raw=true" width="400">

å…¶ä¸­ç¬¬ä¸€ç§è¢«ç§°ä¸ºâ€œåŒå¡”æ¨¡å‹(dual-encoder)â€ï¼ŒQuery å’Œ Document åˆ†åˆ«åš Embeddingsï¼Œç‰¹å¾äº¤äº’åªå‘ç”Ÿåœ¨æœ€åï¼Œä¸€èˆ¬æ˜¯ç®€å•çš„ç®—ä¸¤ä¸ªEmbeddingsçš„ä½™å¼¦è·ç¦»ã€‚
Document çš„ Embeddings å¯ä»¥ç¦»çº¿é¢„è®¡ç®—ï¼ŒEmbeddingså­˜å‚¨é‡æ¯”è¾ƒå°ï¼Œçº¿ä¸Šåªéœ€è¦ç®—ç®€å•çš„ä½™å¼¦è·ç¦»ï¼Œè¿˜å¯ä»¥ç”¨ANN (Approximate Nearest Neighbor) åŠ é€Ÿã€‚
å¯ä»¥scaleï¼Œä»å‡ ä¸‡ç”šè‡³ç™¾ä¸‡åƒä¸‡æ–‡æ¡£å¬å›ã€‚

åé¢å‡ ç§ Query å’Œ Document ç‰¹å¾ä»æµ…å±‚å¼€å§‹èåˆï¼Œæ•ˆæœè‚¯å®šæ¯”ç¬¬ä¸€ç§â€œåŒå¡”æ¨¡å‹â€å¥½ï¼Œä½†ä»æµ…å±‚å¼€å§‹èåˆæ„å‘³ç€éœ€è¦Queryå’ŒDocumentä¸¤ä¸¤è®¡ç®—ï¼Œè®¡ç®—é‡ä¹Ÿå¾ˆå¤§ï¼Œæ²¡åŠæ³•scaleã€‚

retrieval rerank ä¸¤é˜¶æ®µæ£€ç´¢ï¼Œç¬¬ä¸€é˜¶æ®µå…ˆç”¨åŒå¡”æ¨¡å‹å¤§é‡å¬å›æ¯”å¦‚ï¼Œtop100,ï¼Œç¬¬äºŒé˜¶æ®µå°†å¬å›å€™é€‰é›†å’ŒQueryä¸¤ä¸¤è®¡ç®—ï¼Œå¾—åˆ°æ›´ç²¾ç¡®çš„æ£€ç´¢æ’åºã€‚

# Survey
- Sun, 27 Nov 2022 [Dense Text Retrieval based on Pretrained Language Models: A Survey](https://arxiv.org/abs/2211.14876)
  - 2022 å¹´å¯¹äº Dense Text Retrieval çš„ Survey å·²ç»æœ‰ 351 å¼•ç”¨
  - å…¶ä¸­åŒ…æ‹¬ 6 ç¯‡ ä¹‹å‰çš„ Surveyã€‚è¡Œå§
- Mon, 27 May 2024 [Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark](https://arxiv.org/abs/2406.01607)
- Mon, 28 Jul 2025 [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)

# Traditional Retrieval(Sparse lexical search algorithms)
è™½ç„¶ dense retrieval ä»2020å¹´å¼€å§‹å˜æˆæ£€ç´¢æ¨¡å‹çš„ä¸»æµï¼Œä¼ ç»Ÿæ£€ç´¢ç®—æ³•æ¯”å¦‚ BM25 å¯¹å…³é”®è¯ã€ä¸“ä¸šåè¯ç­‰å¬å›æ•ˆæœæ¯”è¾ƒå¥½ï¼Œä»ç„¶æ˜¯ dense retrieval æœ‰æ•ˆçš„è¡¥å……ã€‚
- 2009 [The probabilistic relevance framework: Bm25 and beyond](https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf)
  - Stephen Robertson, Hugo Zaragoza, et al. 2009.
  - Foundations and Trends in Information Retrieval, 3(4):333â€“389.
- 2020 [Which bm25 do you mean? a large-scale reproducibility study of scoring variants](https://link.springer.com/chapter/10.1007/978-3-030-45442-5_4)
  - Chris Kamphuis, Arjen P De Vries, Leonid Boytsov, and Jimmy Lin. 2020. : 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14â€“17, 2020, 
  - In Advances in Information Retrieval Proceedings, Part II 42, pages 28â€“34. Springer.
- Thu, 4 Jul 2024 [BM25S: Orders of magnitude faster lexical search via eager sparse scoring](https://arxiv.org/abs/2407.03618)
  - We introduce BM25S, an efficient Python-based implementation of BM25 that only depends on Numpy and Scipy.
  - It also achieves considerable speedups compared to highly optimized Java-based implementations, which are used by popular commercial products.
  - å•çº¿ç¨‹è·Ÿ lucene æœ‰çš„ä¸€æ‹¼ï¼Œé€Ÿåº¦çœŸçš„å¾ˆå¿«

# Hybrid Retrievers
- Fri, 22 Mar 2024 [Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers](https://arxiv.org/abs/2404.07220)
- Mon, 1 Jul 2024 [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/abs/2407.01219)
  - Taking efficiency into consideration, Hybrid Search combines sparse retrieval (BM25) and dense retrieval (Original embedding) and achieves notable performance with relatively low latency.

# History
è¯¡å¼‚çš„æ•°æ®é›†æ„å»ºå’Œæµ‹è¯•æ–¹æ³•æ€ä¹ˆæ¥çš„ã€‚
- Fri, 31 Mar 2017 [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051)
  - Machine reading at scale (MRS).
    - Using Wikipedia articles as the knowledge source causes the task of question answering (QA) to combine the challenges of both large-scale open-domain QA and of machine comprehension of text.
  - In this paper, we show how multiple existing QA datasets can be used to evaluate MRS by requiring an open-domain system to perform well on all of them at once.
  - In the following we describe our system DrQA for MRS which consists of two components: 
    - (1) the Document Retriever module for finding relevant articles and 
    - (2) a machine comprehension model, Document Reader, for extracting answers from a single document or a small collection of documents.
    - Retriever + Reader æ¶æ„è‡³å°‘åœ¨2017å¹´å·²ç»æå‡º
  - Document Retriever
    - we use an efficient (non-machine learning) document retrieval system to first narrow our search space and focus on reading only articles that are likely to be relevant. 
  - Document Reader
    - Our Document Reader model is inspired by the recent success of neural network models on machine comprehension tasks, in a similar spirit to the AttentiveReader described in (Hermann et al., 2015; Chen et al., 2016).
    - RNN model predicting the two ends of the span. 
  - Experimental Setup:
    - (WebQuestions 2013(WQ), CuratedTREC 2015(TREC), WikiMovies 2016, SQuAD v1.1 2016)
    - Evidence Corpus
      - We use the 2016-12-21 dump of English Wikipedia for all of our full-scale experiments as the knowledge source used to answer questions.
- Fri, 5 May 2017 [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364)
  - BiLSTM with mean/max pooling æˆ‘çš„å¤©
  - head (u, v, |u âˆ’ v|, u âˆ— v) -> fully-connected layers -> 3-way softmax
- Sat, 1 Jun 2019 [Latent Retrieval for Weakly Supervised Open Domain Question Answering](https://arxiv.org/abs/1906.00300)
  - å°† ORQA åˆ—åœ¨å†å²é‡Œï¼Œè€Œå°† DPR åˆ—ä¸º Retrieval æ¨¡å‹çš„ç¬¬ä¸€ç¯‡æ˜¯å¦æœ‰å¤±åé¢‡ï¼Ÿ
  - open domain question answering (QA)
    - Due to recent advances in reading comprehension systems, 
    - there has been a revival of interest in open domain question answering (QA), 
    - where the evidence must be retrieved from an open corpus, rather than being given as input. 
    - This presents a more realistic scenario for practical applications.
  - However, QA is fundamentally different from IR (Singh, 2012).
  - Retriever component
    - Query å’Œ Documentä½¿ç”¨ä¸åŒæ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¦è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼Œinner product ä½œä¸ºç›¸ä¼¼å‡½æ•°ã€‚Embeddings æŠ•å½±åˆ°ç»´åº¦ 128ã€‚
    - Retrieveræ¨¡å‹ç»“æ„è·ŸDPRå¤§å·®ä¸å·®
  - Reader component 
    - The reader is a span-based variant of the reading comprehension model proposed in Devlin et al. (2018):
    - Readeræ¨¡å‹ç»“æ„è·ŸDPRå¤§å·®ä¸å·®
  - Inverse Cloze Task
    - Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task.
    - è™½ç„¶Retriever componentæ¨¡å‹ç»“æ„å’ŒDPRå¤§å·®ä¸å·®ï¼Œå·®å¼‚åœ¨ lossï¼ŒDPR ä½¿ç”¨ Metric Learning æ–¹æ³•ã€‚
  - Experimental Setup:
    - Evidence Corpus
      - We use the English Wikipedia snapshot from December 20, 2018 as the evidence corpus.
      - The corpus is greedily split into chunks of at most 288 wordpieces based on BERTâ€™s tokenizer, while preserving sentence boundaries.
    - We train and evaluate on data from 5 existing question answering or reading comprehension datasets.
    - (Natural Questions 2019(NQ), WebQuestions 2013(WQ), CuratedTREC 2015(TREC), TriviaQA 2017, SQuAD v1.1 2016) ä¸ DPR ç›¸åŒ
    - we convert them to open formats, following DrQA (Chen et al., 2017).
      - Natural Questions 
        - we only keep questions with short answers and discard the given evidence document. 
        - Answers with many tokens often resemble extractive snippets rather than canonical answers, so we discard answers with more than 5 tokens.
      - WebQuestions
        - The answers are annotated with respect to Freebase, but we only keep the string representation of the entities.
      - CuratedTrec
      - TriviaQA
        - We use their unfiltered set and discard their distantly supervised evidence
      - SQuAD
    - | Dataset           | Train | Dev  | Test  |
      |-------------------|-------|------|-------|
      | Natural Questions | 79168 | 8757 | 3610  |
      | WebQuestions      | 3417  | 361  | 2032  |
      | CuratedTrec       | 1353  | 133  | 694   |
      | TriviaQA          | 78785 | 8837 | 11313 |
      | SQuAD             | 78713 | 8886 | 10570 |
  - Main Results
    - BM25 + BERT åœ¨ TriviaQA SQuAD æ•ˆæœå¥½
    - ORQA(ours) åœ¨ Natural Questions, WebQuestions, CuratedTrec ä¸Šæ•ˆæœå¥½
- Thu, 22 Aug 2019 [Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering](https://arxiv.org/abs/1908.08167)
  - previous work defines passages as articles, paragraphs, or sentences. However, the question of proper granularity of passages is still underexplored.
    - (RAG chucking çš„ç²’åº¦é—®é¢˜ä» 2019 å¹´ä¸€ç›´è®¨è®ºåˆ° 2024 å¹´ï¼Œbaseæ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œä¸Šä¸‹æ–‡èƒ½åŠ›æ¯å¢å¼ºä¸€æ¬¡éƒ½ä¼šé‡æ–°è®¨è®ºä¸€æ¬¡
  - we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%.
    - We set the window size as 100 words, and the stride as 50 words(half the window size). 
    - è®°ä½è¿™ä¸ª 100 words as passages
  - Passage ranker reranks all retrieved passages, and selects a list of high-quality passages for the multi-passage BERT model.
    - First, the retriever returns top-100 passages for each question. Then, the passage ranker is employed to rerank these 100 passages. Finally, multi-passage BERT takes top30 reranked passages as input to pinpoint the final answer. 
    - (ranker ç†å¿µå‡ºç°çš„æ—¶é—´ä¹Ÿéå¸¸æ—©
  - we use the 2016-12-21 English Wikipedia dump. Following DrQA 2017

# Retrieval(Embeddings) model
- Tue, 27 Aug 2019 [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
  - sbert æ˜¯ Siamese Dual Encoder ä¹Ÿå°±æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼ŒDPR æ˜¯ Asymmetric Dual Encoder ä¹Ÿå°±æ˜¯ä¸¤ä¸ªæ¨¡å‹ã€‚SDEæ•ˆæœæ¯”ADEå¥½ï¼Œå½“ç„¶è¿™æ˜¯åè¯äº†ã€‚
  - è¿™ç¯‡è®ºæ–‡æ²¡æœ‰å‘open domain question answering æ–¹å‘å‘åŠ›ï¼Œéå¸¸å¯æƒœï¼Œå¯¹æ¯ä¸ªæ•°æ®é›†éƒ½ Supervised Fine-tuning
  - è¯„ä¼°æ–¹æ³•ç°åœ¨æ²¡ä»€ä¹ˆè®ºæ–‡followï¼Œæ‰€ä»¥ä¸€èˆ¬éƒ½æ²¡æœ‰è·ŸDPRè¿›è¡Œæ¯”è¾ƒã€‚å½“ç„¶sentence-transformersçš„åæ°”ä¹Ÿå¾ˆå¤§äº†ã€‚
  - [sentence-transformers](https://github.com/UKPLab/sentence-transformers/)
  - [Document](https://www.sbert.net/)
  - Objective Function
    - Classification Objective Function o = softmax(Wt(u, v, |u âˆ’ v|))
    - Regression Objective Function cosine-sim(u, v)
    - Triplet Objective Function max(||sa âˆ’ sp|| âˆ’ ||sa âˆ’ sn|| + s, 0)
  - Ablation Study
    - Pooling Strategy MEAN 80.78 > CLS 79.07 > MAX 79.80
    - Concatenation (u, v, |u âˆ’ v|) 80.78 æ•ˆæœå±…ç„¶è¿˜ä¸é”™
- Fri, 10 Apr 2020 [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)
  - DPR è®ºæ–‡æå‡ºçš„æ•´ä¸ªä½“ç³»ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€è®­ç»ƒã€åœ¨çº¿æ¨ç†ï¼Œéƒ½è·Ÿç°åœ¨ä¸»æµç›¸è¿‘ï¼Œæ•°æ®é›†çš„å¤„ç†æ–¹å¼æœ€æ–°è®ºæ–‡è¿˜åœ¨followï¼Œåœ¨å‡ ä¸ªæ•°æ®é›†ä¸Šçš„ç»“æœæœ€æ–°è®ºæ–‡ä»ä½œä¸ºbaselineæ¯”è¾ƒã€‚é‚£å°±å¤šå†™ä¸€ç‚¹ã€‚
  - Transformer 2017å¹´å‘å¸ƒï¼ŒBERT 2019å¹´å‘å¸ƒï¼Œå¼€å§‹åˆ·nlpä»»åŠ¡ã€‚çœŸæ˜¯å‹ƒå‹ƒç”Ÿæœºä¸‡ç‰©ç«å‘çš„æ—¶ä»£ã€‚
  - ä½¿ç”¨BERTé¢„è®­ç»ƒæ¨¡å‹ï¼ŒEmbeddings ç»´åº¦ 768ï¼ŒQuery å’Œ Documentä½¿ç”¨ä¸åŒæ¨¡å‹ï¼Œä¹Ÿå°±æ˜¯è¦è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼Œinner product ä½œä¸ºç›¸ä¼¼å‡½æ•°ï¼Œ loss function ä½¿ç”¨ negative log likelihood
  - DPR performs consistently better than BM25 on all datasets. ï¼ˆDense Retrieval ç™»ä¸Šå†å²çš„èˆå°
  - Experimental Setup:
    - Evidence Corpus
      - English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. Following (Lee et al., 2019), Following DrQA 2017 
      - We then split each article into multiple, disjoint text blocks of 100 words as passages, serving as our basic retrieval units. Following (Wang et al., 2019) 
      - è®°ä½è¿™ä¸ª 100 words as passages
    - five QA datasets(Natural Questions 2019(NQ), TriviaQA 2017, WebQuestions 2013(WQ), CuratedTREC 2015(TREC), SQuAD v1.1 2016)
    - Selection of positive passages
      - TREC, WebQuestions and TriviaQA:  we use the highest-ranked passage from BM25 that contains the answer as the positive passage. If none of the top 100 retrieved passages has the answer, the question will be discarded.
      - SQuAD and Natural Questions: since the original passages have been split and processed differently than our pool of candidate passages, we match and replace each gold passage with the corresponding passage in the candidate pool.
  - Ablation Study on Model Training
    - Sample efficiency 
      - a dense passage retriever trained using only 1,000 examples already outperforms BM25.
      - Adding more training examples (from 1k to 59k) further improves the retrieval accuracy consistently.
    - In-batch negative training
      - re-using gold passages from the same batch as negatives can make the computation efficient while achieving great performance.
      - a batch size of 128 and one additional BM25 negative passage per question
      - in-batch negative training improves the results substantially. As a result, accuracy consistently improves as the batch size grows. 
      - â€œhardâ€ negative passages that have high BM25 scores given the question, but do not contain the answer string (the bottom block)
      - We find that adding a single BM25 negative passage improves the result substantially while adding two does not help further.
    - Impact of gold passages 
    - Similarity and loss
      - L2 performs comparable to dot product, and both of them are superior to cosine.
      - Our experiments show that using triplet loss does not affect the results much.
    - Cross-dataset generalization
    - Qualitative Analysis
      - Term-matching methods like BM25 are sensitive to highly selective keywords and phrases
      - while DPR captures lexical variations or semantic relationships better.
    - Run-time Efficiency
      - BM25+Lucene vs DPR+FAISS
  - End-to-end QA System
    - The probabilities of a token being the starting/ending positions of an answer span and a passage being selected. (è¿™ç§é¢„æµ‹ç­”æ¡ˆä½ç½®çš„ reading comprehension (RC) ä»»åŠ¡å·²ç»é€€å‡ºå†å²èˆå°)
    - measured by exact match with the reference answer after minor normalization as in (Chen et al., 2017; Lee et al., 2019)  (EMçš„è¯„åˆ¤æ ‡å‡†ä¸€ç›´å»¶ç»­ä¸‹æ¥ï¼‰
    - higher retriever accuracy typically leads to better final QA results
    - Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks.
    - Retrieval + generation è¿™å·²ç»å¾ˆRAGäº†
  - Main Results
    - DPR å…¨é¢è¶…è¶Š BM25ï¼Œmulti-datasetè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ•ˆæœæ›´å¥½ï¼ŒDPR+BM25ç›¸äº’è¡¥å……æ•ˆæœç¨å¾®æœ‰æé«˜
    - <img src="https://github.com/noooop/noooop.github.io/blob/main/applications/rag/dpr2.png?raw=true" width="400">
    - <img src="https://github.com/noooop/noooop.github.io/blob/main/applications/rag/dpr1.png?raw=true" width="400">
- Fri, 16 Oct 2020 [RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2010.08191)
  - ç™¾åº¦çš„ ERNIE çš„ RocketQA ä¼šä¸ä¼šå› ä¸ºä½¿ç”¨ PaddlePaddle è€Œä¸æ˜¯ pytorch è¢«ä½ä¼° 
- Sun, 18 Apr 2021 [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)
  - We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, 
  - with only standard dropout used as noise. 
  - This simple method works surprisingly well, performing on par with previous supervised counterparts. 
  - We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse.
    - æœ‰æ„æ€ã€‚ ä½† unsupervised æ•ˆæœç”šè‡³ä¸å¦‚ BM25 ï¼ˆè§E5è®ºæ–‡ï¼‰
- Thu, 19 Aug 2021 [Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models](https://arxiv.org/abs/2108.08877)
  - Google çš„ sentence embeddings from text-to-text transformers (ST5)
    - ST5-Enc Base 110M, Large 335M, 3B 1.24B, 11B 4.8B
    - ST5-EncDec Base 110M, Large 335M, 3B 3B, 11B 11B
  - ST5-Enc mean æ•ˆæœæ¯” ST5-EncDec first å’Œ ST5-Enc first æ•ˆæœå¥½ã€‚
  - encoder-only å¯¹äº Retrieval ä»»åŠ¡å·²ç»è¶³å¤Ÿäº†
- Wed, 15 Dec 2021 [Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/abs/2112.07899)
  - Google çš„ Generalizable T5-based dense Retrievers (GTR)
  - Base 110M, Large 335M, XL 1.24B, XXL 4.8B
- Thu, 16 Dec 2021 [Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118)
  - Finally, we also consider additional data augmentations such as random word deletion, replacement or masking. 
  - We use these perturbations in addition to random cropping.
  - MoCo
  - Contriever æ¯” SimCSE æ•ˆæœå¥½ï¼Œæ¥è¿‘BM25 
- Thu, 14 Oct 2021 [RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking](https://arxiv.org/abs/2110.07367)
  - ç™¾åº¦çš„ ERNIE çš„ RocketQA ä¼šä¸ä¼šå› ä¸ºä½¿ç”¨ PaddlePaddle è€Œä¸æ˜¯ pytorch è¢«ä½ä¼° 
- Thu, 14 Apr 2022 [Exploring Dual Encoder Architectures for Question Answering](https://arxiv.org/abs/2204.07120)
  - Dual encoders have been used for questionanswering (QA) and information retrieval (IR) tasks with good results.
  - Previous research focuses on two major types of dual encoders,
    - Siamese Dual Encoder (SDE), with parameters shared across two encoders,  (SBERT (Reimers and Gurevych, 2019), ST5 (Ni et al., 2021b) 
    - and Asymmetric Dual Encoder (ADE), with two distinctly parameterized encoders. (DPR (Karpukhin et al., 2020), DensePhrases (Lee et al., 2021a) 
  - we show that SDE performs significantly better than ADE.
  - We further propose three different improved versions of ADEs by sharing or freezing parts of the architectures between two encoder towers.
    - We find that sharing parameters in projection layers would enable ADEs to perform competitively with or outperform SDEs.
    - We further explore and explain why parameter sharing in projection layer significantly improves the efficacy of the dual encoders, by directly probing the embedding spaces of the two encoder towers with t-SNE algorithm.
  - Main Results
    - By directly probing the embedding space, we demonstrate that the shared projection layers in SDE and ADE-SPL maps the embeddings of the two encoder towers into coinciding parameter spaces, 
    - which is crucial for improving the retrieval quality. Therefore, we recommend to share the projection layers between two encoders of ADEs in practice.
    - <img src="https://github.com/noooop/noooop.github.io/blob/main/applications/rag/edea.png?raw=true" width="400">
    - è¿™ä¸ªç»“è®ºå¯ä»¥æ³›åŒ–åœ¨æ•´ä¸ªåœ¨ Metric Learning é—®é¢˜
- Thu, 26 May 2022 [Matryoshka Representation Learning](https://arxiv.org/abs/2205.13147)
  - æ”¯æŒå¤šä¸ªå‘é‡ç»´åº¦
- Wed, 7 Dec 2022 [Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533)
  - å¾®è½¯çš„E5
  - We pre-train on our proposed text pair dataset for three model sizes: E5small, E5base and E5large initialized from MiniLM, bert-base-uncased, and bert-large-uncased-whole-wordmasking respectively
  - Training Recipe (two-stage training) 
    - Weakly-Supervised Contrastive Pre-training 
      - æ„å»º CCPairs æ•°æ®é›†
      - we use mined hard negatives and knowledge distillation from a cross-encoder (CE) teacher model
    - Supervised Fine-tuning
      - MS-MARCOï¼Œ NQï¼Œ NLI
      - We reuse the mined hard negatives and re-ranker scores from SimLM [58] for the first two datasets.
  - Evaluation
    - BEIRï¼Œ MTEB
    - Weakly-Supervised Contrastive Pre-training 
      - E5-PT base outperforms the classic BM25 algorithm by 1.2 points. 
      - To the best of our knowledge, this is the first reported result that an unsupervised model can beat BM25 on the BEIR benchmark.
      - BM25 41.7ï¼Œ SimCSE 20.3ï¼ŒE5-PT large 44.2
      - E5-PT large ä¹Ÿå°±ç•¥å¥½äº BM25
    - Supervised Fine-tuning
      - Most datasets benefit from supervised finetuning
      - Supervised models E5large æ¯”ä¹‹å‰ GTRxxlã€Sentence-T5xxlå¼º
      - Since the difference between BERT-FT base and E5 base is that BERT-FT base only has fine-tuning stage, 
      - their performance gap demonstrates the usefulness of contrastive pre-training on our proposed CCPairs dataset.
      - Is Contrastive Pre-training Necessary? ï¼ˆImproving Text Embeddings with Large Language Modelsï¼‰
  - Weakly-Supervised Contrastive Pre-training + Supervised Fine-tuning ç§°ä¸º sota æ¨¡å‹çš„æ ‡é…
  - BM25 vs Dense Retrieval
    - The answer is likely â€œnot yetâ€. BM25 still holds
obvious advantages in terms of simplicity, efficiency, and interpretability. For long-tail domains such
as Trec-Covid [55] and retrieval tasks that involve long documents (Touche-2020) [4] or rely heavily
on exact lexical match (Fever) [54], further research efforts are still necessary to improve current
dense retrievers.
- Thu, 20 Jul 2023 [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models]
  - Architecture
    - T5 architecture ä¼°è®¡æ˜¯å¯¹æ ‡ Google çš„ GTR
  - Pairwise Data Preparation
    - De-Duplicationï¼Œ Language Filteringï¼Œ Consistency Filtering
  - Triplet Data Preparation
    - we leverage the ms-marco-MiniLM-L-6-v2 model5 to verify whether the difference in retrieval scores determined by the model exceeds a threshold r(q, p) âˆ’ r(q, n) > Îº, with threshold Îº = 0.2, and eliminate all other pairs.
    - This methodology draws inspiration from the de-noising strategy proposed in [Qu et al., 2021].
  -  Negation Data Preparation
    - This dataset, based on positive pairs from the SNLI dataset and negatives created with GPT-3.5
  - Training
    - Training on Pairwise Data
    - Training on Triplet Data
- Mon, 7 Aug 2023 [Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/abs/2308.03281)
  - Alibaba çš„ GTE
  - Architecture
    - Bert 512ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ mean poolingï¼Œ InfoNCE loss
  - Training Recipe (two-stage training) 
    - Weakly-Supervised Contrastive Pre-training (from bert pre-train)
      - Weakly supervised text relevance data is readily available in publicly accessible web sources, such as the inherent connection between queries and answers on QA forums.
    - Supervised Fine-tuning
      - we use relatively lower-sized datasets with human annotation of the relevance between two pieces of text and optional hard negatives mined by an extra retriever to form text triples.
  - Improved Contrastive Loss 
    - in which the first two terms are used for query to document contrast, where as the last two terms are used for the inverse.
    - s(qi,qj) å’Œ s(dj ,di) ä¹Ÿç”¨èµ·æ¥
    - The temperature Ï„ is fixed to 0.01 in this work.
  - Evaluation
    - BEIRï¼šwe find that our base size model significantly outperforms the models with comparable size, like SimCSE, Contriever and E5. Our base model is comparable to E5large without using human supervision.
  - Analysis
    - Number of Training Datasets
      - The results presented in Figure 3a demonstrate that the inclusion of more diverse data sources consistently enhances model performance during both the pre-training and finetuning stages.
    - Pre-training Batch Size
      - model performance saturates at around a batch size of ten thousands.
    - Number of Model Parameters
      -  It can be observed that as the model size grows exponentially, the model performance also improves linearly.
    - Influence of Different Training Stages
      - PT å’Œ FT éƒ½æœ‰ç”¨
    - Ablation of the Contrastive Objective
      - FT ä¸Šç¨å¾®æœ‰ä¸€ç‚¹æé«˜ï¼Œä½†ä¹‹åçš„mGTEå¹¶æ²¡æœ‰ä½¿ç”¨
- Thu, 14 Sep 2023 [C-Pack: Packed Resources For General Chinese Embeddings](https://arxiv.org/abs/2309.07597)
  - architecture, bert small, base, large  512 é•¿åº¦
  - Training Recipe (three-stage training)
    - MAE Pre-Training ä½¿ç”¨ Wudao corpora ä»é›¶å¼€å§‹è®­ç»ƒï¼ŒWe leverage the MAE-style approach presented in RetroMAE
    - General purpose fine-tuning
      - The pre-trained model is finetuned on C-MTP (unlabeled) via contrastive learning
      - we purely rely on in-batch negative samples [25] and resort to a big batch size (as large as 19,200) to improve the discriminativeness of the embedding.
    - Task-specific fine-tuning
      -  The hard negative sample is mined from the taskâ€™s original corpus, following the ANN-style sampling strategy in [61].
  - Detailed Analysis
    - pretrain & finetune
      - a mixture of high-quality and diversified labeled data is able to bring forth substantial and comprehensive improvements for a pre-trained embedding model.
    - batch size
      - By making a parallel comparison between bz: 256, 2028, 19,200, we observe consistent improvement in embedding quality with the expansion of batch size (noted as bz).
    - Instruct
      - using instructions may substantially contribute to the quality of task-specific fine-tuning
- Fri, 22 Sep 2023 [AnglE-optimized Text Embeddings](https://arxiv.org/abs/2309.12871)
  - AnglE-BERT & AnglE-LLaMA2-7B
  - COSINE OBJECTIVE å¯èƒ½é¥±å’Œï¼Œæ‰€ä»¥æå‡º ANGLE OBJECTIVE 
  - åº¦é‡å­¦ä¹  angular space ï¼ˆAngular Contrastive Learningï¼‰ çœŸçš„åªæœ‰ä¸€ç¯‡è®ºæ–‡å—
  - applies LLMs as data annotators to label the pseudo-supervised data for AnglE training.
    -  For the STS task, we use the prompt
â€œYou are a highly smart same-meaning/opposite-meaning sentence-generating system. Your job is
to generate {size} synonymous/antonym sentences of a given input sentence. Input sentence: {text}.
Output:â€ to generate positive/negative pairs. {size} and {text} are placeholders for the generated
size and the input text, respectively.
    - æˆ‘çš„å¤©ï¼Œgenerate positive/negative pairs
- Thu, 12 Oct 2023 [Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/abs/2310.08319)
  - LLM as Retrieval +1 
  - RepLLaMA & RankLLaMA ä½¿ç”¨ LLaMA-2-7B å’Œ LLaMA-2-13B
- Mon, 30 Oct 2023 [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents](https://arxiv.org/abs/2310.19923)
  - Architecture
    - BERT with ALiBi, GEGLU, BF16, mean pooling
    - bert-small 33M, bert-base 137M
  - Training
    - Pre-training the Backbone, (C4 + 30% MLM)
    - First Fine-tuning with Text Pairs, InfoNCE 
    - Second Fine-tuning with Hard Negatives
      - includes one positive and 15 negative instances
      - To ensure that hard negative passages are indeed less relevant than the annotated relevant ones, we employ a cross-encoder model to validate that their relevance score is indeed lower.
- Fri, 29 Dec 2023 [MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining](https://arxiv.org/abs/2312.17482)
  - è¿™ç¯‡æ›´å¤šè®²çš„æ˜¯é¢„è®­ç»ƒbert 
  - This architecture combines FlashAttention [11], ALiBi [44], Gated Linear Units[12, 50], a dynamic unpadding module [66], and low precision LayerNorm.
- Sun, 31 Dec 2023 [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368)
  - LLM as Retrieval +1
  - we introduce a novel and simple method for obtaining high-quality text embeddings using only synthetic data and less than 1k training steps.
    - we design a two-step prompt template that first prompts LLMs brainstorm a list of tasks, and then generates a concrete example conditioned on the task definition.
    - query positive_document hard_negative_document å…¨åˆæˆå•Šï¼Ÿï¼Ÿè¿™ä¹Ÿå¤ªé‡äº†
    - w/ synthetic only 63.1ï¼Œw/ synthetic + msmarco 64.5ï¼Œw/o synthetic data 64.6ï¼Œfull data 66.6
    - ä½ ç¡®å®šä½ çš„åˆæˆæ•°æ®æœ‰ç”¨ï¼Ÿ
  - 5.1 Is Contrastive Pre-training Necessary?
    - contrastive pre-training benefits XLM-Rlarge, enhancing its retrieval performance
    - However, for Mistral-7B based models, contrastive pre-training has negligible impact on the model quality
  - å¯¹æ¯” E5 å’Œ E5 mistral-7bï¼Œ 7B åŠæ‰“ 330Mï¼Ÿ
- Fri, 2 Feb 2024 [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://arxiv.org/abs/2402.01613)
  - NomicBertModel æ¶æ„æ¯”è¾ƒç°ä»£ bert_with_rope
  - https://huggingface.co/nomic-ai/nomic-bert-2048
  - Architecture 
    - rotary + SwiGLU + Flash Attentionï¼Œ12å±‚768ç»´ï¼Œ137Bï¼Œè®­ç»ƒé•¿åº¦2048
    - Dynamic NTK interpolation at inference to scale to 8192 sequence length
  - Training Recipe (three-stage training)
    - MLM pre-train (from scratch)
      - Masked Language Modeling Pretraining (BooksCorpus and Wikipedia)ï¼Œé•¿åº¦ 2048, 30% masking rate
      - Additionally, we opt for SwiGLU versus GeGLU like proposed in Portes et al. (2023) as runtime is roughly 25% faster for SwiGLU using the Flash Attention repository.
    - Weakly-Supervised Contrastive Pretraining
      - Consistency Filtering
        - Since many of these datasets may contain noisy examples, we employ consistency filtering to remove the potential false positives in the dataset
    - Supervised Contrastive Fine-tuning
      - data (MSMarco, NQ, NLI....) 
      - For other non-retrieval datasets, we randomly sample negatives among the corpus in place of mining hard negatives as we found that mining did not improve performance.
      - We also found that training for multiple epochs hurts performance.
      - Instead of choosing the first N negatives, we randomly sampled the mined negatives. 
      - We found this to improve performance as some of the mined negatives introduced false negatives.
  - Evaluate
    - test on MTEBï¼ŒJinaâ€™s Long Context Benchmarkï¼ŒLoCo
- Sun, 4 Feb 2024 [ä¸ºRAGè€Œç”Ÿ-BCE embeddingæŠ€æœ¯æŠ¥å‘Š](https://zhuanlan.zhihu.com/p/681370855)
  - åŒæ—¶å‘å¸ƒ embedding å’Œ reranker ç¡®å®æ˜¯ä¸ºRAGè€Œç”Ÿï¼Œä½†512åºåˆ—é•¿åº¦æ˜¾ç„¶æ˜¯æ¯é¢„æµ‹å•æ¥ä¸‹æ¥è½°è½°çƒˆçƒˆçš„é•¿ä¸Šä¸‹æ–‡æ—¶ä»£
  - äºŒé˜¶æ®µæ£€ç´¢å™¨ï¼ˆTwo-stage Retrieverï¼‰â€œç¦»çº¿â€çš„Embeddingæ­é…â€œåœ¨çº¿â€çš„Reranker
  - éš¾è´Ÿæ ·ä¾‹æŒ–æ˜ï¼Ÿ
    - æˆ‘ä»¬åœ¨è®­ç»ƒEmbeddingæ¨¡å‹æ—¶å‘ç°ï¼Œè¿‡éš¾çš„è´Ÿæ ·æœ¬å¯¹æ¨¡å‹è®­ç»ƒæœ‰æŸå®³ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šä½¿æ¨¡å‹â€œå›°æƒ‘â€ï¼Œ[å½±å“æ¨¡å‹æœ€ç»ˆæ€§èƒ½](https://kexue.fm/archives/8847#%E9%9A%BE%E6%90%9E%E7%9A%84%E9%98%88%E5%80%BC)ã€‚
    - åœ¨å¤§é‡çš„è¯­æ–™åº“ä¸­ï¼Œè„±ç¦»äººå·¥æ ¡éªŒçš„è‡ªåŠ¨åŒ–éš¾è´Ÿæ ·ä¾‹æŒ–æ˜ï¼Œéš¾å…ä¼šâ€œæŒ–åˆ°æ­£ä¾‹â€ã€‚
    - å…¶å®æ‰€è°“çš„â€œæ­£ä¾‹â€å’Œâ€œéš¾è´Ÿæ ·ä¾‹â€æ˜¯æ ¹æ®ä½ ä¸šåŠ¡çš„å®šä¹‰æ¥çš„ã€‚
    - æ‰€ä»¥å›å½’ä¸šåŠ¡ç›®æ ‡å’Œå¥½çš„æ£€ç´¢å™¨çš„â€œè¯„åˆ¤æ ‡å‡†â€ï¼ŒEmbeddingæ¨¡å‹åº”è¯¥èƒ½å°½é‡å¬å›ç›¸å…³ç‰‡æ®µï¼Œä¸è¦å°†Rerankerè¦å¹²çš„ç²¾æ’ä»»åŠ¡å¼ºå‹åœ¨Embeddingèº«ä¸Šï¼Œâ€œè¶Šä¿ä»£åº–â€ç»ˆç©¶ä¼šå®³äº†å®ƒã€‚
- Mon, 5 Feb 2024 [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216)
  - BAAI çš„ BGE M3
  - ä½¿ç”¨ XLM-RoBERTa æ¶æ„è¿˜æ˜¯æ¯”è¾ƒä¼ ç»Ÿï¼Œ 23 å±‚ 1024ç»´
  - we introduce a new embedding model called M3-Embedding Supervised models
    - Multi-Linguality:  It provides a uniform support for the semantic retrieval of more than 100 working languages. Enables both multilingual retrieval within each language and crosslingual retrieval between different languages.
    - Multi-Functionality: It can simultaneously accomplish the three common retrieval functionalities: dense retrieval, multi-vector retrieval, and sparse retrieval.
    - Multi-Granularity:  Besides, it is also capable of processing inputs of different granularities, spanning from short sentences to long documents of up to 8,192 tokens
  - Related Work
    - powerful text encoders bert 2019, DPR 2020, ST5 2022
    - negative sampling (Xiong et al., 2020; Qu et al.,2021) 
    - knowledge distillation (Hofstatter et al. Â¨ , 2021; Ren et al., 2021; Zhang et al., 2021a). 
    - ä¹‹å‰çš„ Dense Retrievalï¼šContriever (Izacard et al., 2022), LLM-Embedder (Zhang et al., 2023a), E5 (Wang et al., 2022), BGE (Xiao et al., 2023), SGPT (Muennighoff, 2022), and Open Text Embedding (Neelakantan et al., 2022),
  - In our work, the following technical contributions are made to optimize the embedding quality. 
    - Firstly, we propose a novel self knowledge distillation framework
      - the [CLS] embedding is used for dense retrieval, while embeddings from other tokens are used for sparse retrieval and multi-vector retrieval.
      - we integrate the relevance scores from different retrieval functions as the teacher signal, which is used to enhance the learning process via knowledge distillation.
    - Secondly, we optimize the batching strategy to achieve a large batch size and high training throughput, which substantially contributes to the discriminativeness of embeddings. 
    - Last but not least, we perform extensive and high-quality data curation. 
      - Our dataset includes three sources: 
        - 1) the extraction of unsupervised data from massive multi-lingual corpora, In total, it brings in 1.2 billion text pairs of 194 languages and 2655 cross-lingual correspondences.
        - 2) we collect relatively small but diverse and high-quality fine-tuning data from labeled corpora. we incorporate 8 datasets, For Chinese, we integrate 7 datasets, For other languages, we leverage the training data from Mr. Tydi (Zhang et al., 2021b) and MIRACL (Zhang et al., 2023c).
        - 3) the synthesization of scarce training data
          - Specifically, we sample lengthy articles from Wikipedia, Wudao (Yuan et al., 2021) and mC4 datasets and randomly choose paragraphs from them. 
          - Then we use GPT3.5 to generate questions based on these paragraphs.
          - ä½¿ç”¨ GPT åˆæˆæ•°æ®è®­ç»ƒæ¨¡å‹å¼€å§‹æˆä¸ºä¸»æµ
      - The three data sources are complement to each other and applied to different training stages, which lays a solid foundation for the versatile text embeddings.
    - Train
      - loss
        - minimize the InfoNCE loss(NCE stands for Noise-Contrastive Estimation)
      - native multi-objective training can be unfavorable to the embeddingâ€™s quality.
        -  we integrate the relevance scores from different retrieval functions as the teacher signal, which is used to enhance the learning process via knowledge distillation.
      - The training process constituta multi-stage workflow
        - the text encoder (an XLM-RoBERTa (Conneau et al., 2020) model adapted by RetroMAE (Xiao et al., 2022) method) is pre-trained with the massive unsupervised data, where only the dense retrieval is trained in the basic form of contrastive learning.
        - The self-knowledge distillation is applied to the second stage, where the embedding model is fine-tuned to establish the three retrieval functionalities.
          - Both labeled and synthetic data are used in this stage, where hard negative samples are introduced for each query following the ANCE method (Xiong et al., 2020).
      - Efficient Batch
        - It also needs to keep the batch size as large as possible(introducing a huge amount of in-batch negatives) to ensure the discriminativeness of text embeddings
        - Particularly, the training data is pre-processed by being grouped by sequence length. When producing a mini-batch, the training instances are sampled from the same group.
        - We iteratively encode each sub-batch using gradient checkpointing (Chen et al., 2016)and gather all generated embeddings.
        - Finally, the embeddings from different GPUs are broadcasted, allowing each device to obtain all embeddings in the distributed environment, 
          - which notably expands the scale of in-bath negative samples.
    - Experiment
      - Multi-Lingual Retrieval
      - Cross-Lingual Retrieval
      - Multilingual Long-Doc Retrieval
    - Ablation study
      - Self-knowledge distillation
      - Impact of multi-stage training
         - | Model (Dense)                | MIRACL | 
           |------------------------------|--------| 
           | Fine-tune                    | 60.5   | 
           | RetroMAE + Fine-tune         | 66.1   | 
           | RetroMAE + Unsup + Fine-tune | 69.2   |
- Thu, 8 Feb 2024 [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672)
  - Initialization: microsoft/Multilingual-MiniLM-L12-H384, xlm-roberta-base, xlm-roberta-large
  - contrastive pre-training + fine-tuning
- Sat, 24 Feb 2024 [OpenAI vs Open-Source Multilingual Embedding Models Choosing the model that works best for your data](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05)
  - Generate a custom Q/A dataset
  - ä»‹ç»äº†ä¸€ç§ç”¨ChatGPTåˆæˆé—®ç­”æ•°æ®é›†æµ‹è¯•æ£€ç´¢æ¨¡å‹çš„æ–¹æ³•
  - ç”¨ChatGPTåˆæˆé—®ç­”æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œç”¨ChatGPTåˆæˆé—®ç­”æ•°æ®é›†æµ‹è¯•æ¨¡å‹çš„ä¸–ç•Œè¾¾æˆäº†
- Mon, 26 Feb 2024 [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829)
  - ä½¿ç”¨ Guided æ¨¡å‹ç§»é™¤ in-batch negative é‡Œé¢çš„å‡è´Ÿï¼Œç›¸å½“äº CONSISTENCY FILTERING
- Mon, 26 Feb 2024 [Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings](https://arxiv.org/abs/2402.17016)
  - å¤šè¯­è¨€ç‰ˆ Jina Embeddings 2ï¼Œ BERT with ALiBi
- Wed, 27 Mar 2024 [Scaling Laws For Dense Retrieval](https://arxiv.org/abs/2403.18684)
  - 24 BERT checkpoints from the original Google release, with model sizes ranging from 0.5 million (BERT-Tiny) to 82 million parameters (BERT-Base)
  - For experiments on Chinese retrieval benchmarks, we selected the ERNIE series
  - <img src="https://github.com/noooop/noooop.github.io/blob/main/applications/rag/sldr.png?raw=true" width="400">
  - æ¯”è¾ƒçš„æ¨¡å‹ç•¥å°ï¼Œ2025å¹´å¤§æ¦‚èƒ½æ‘¸åˆ°æ¨¡å‹å¤§å°çš„ sweet spotï¼Œå†å›çœ‹ä¸Šé¢çš„å›¾ï¼Œå¤šè¯­è¨€æ¨¡å‹ç¡®å®éœ€è¦æ›´å¤šå‚æ•°
    - å•è¯­è¨€å¯¹æ ‡ BERT BASEï¼šL=12ï¼ŒH=768ï¼ŒA=12 110M
    - å¤šè¯­è¨€å¯¹æ ‡ BERT LARGEï¼šL=24ï¼ŒH=1024ï¼ŒA=16 340M
- Fri, 29 Mar 2024 [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327)
  - Training Recipe (two-stage training) 
    - Weakly-Supervised Contrastive Pre-training 
      - ç±»ä¼¼æ„å»º CCPairs æ•°æ®é›†
      - Note that we do not utilize hard negatives during pre-finetuning and utilize the maximum batch size that fits into the device. 
    - Supervised Fine-tuning
      - FRet: Two-Step LLM Distillation
        - LLM-based Diverse Query Generation
          - we employ few-shot prompts to control the diversity of queries
      - LLM-based Positive and Negative Mining
        - we use an existing embedding model1 to retrieve top ğ‘ neighbors ğ‘ƒ from the corpus given a generated query ğ‘. 
        - We then employ the same LLM used for the query generation to rank these retrieved passages based on their relevance to the query
          - query likelihoodï¼Œ  relevance classification
        - we create the FRet dataset, comprised of 6.6M examples, each containing a task, a query, a positive passage, and a negative passage.
  - Analysis
    - LLM as a Labeler
      - we find that using the most relevant passage chosen by an LLM is always better than using the original passage as positive.
    - Diversity of FRet
    - Learning Semantic Similarity and Classification
    - Qualitative Analysis
      - First, we observe that the LLM does generate diverse tasks and queries by conditioning on seed passages ğ‘ seed
      - Second, the table highlights the LLMâ€™s ability to find a passage (ğ‘1) that provides a more direct and relevant answer to the generated query than the seed passage (ğ‘seed)
      - Furthermore, LLM-ranked hard negatives make a challenging task of understanding nuanced differences.
      - These examples demonstrate how the 2-step LLM distillation process effectively brings the LLMâ€™s diverse domain knowledge and global ranking preferences into the text embedding model.
- Tue, 9 Apr 2024 [LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders](https://arxiv.org/abs/2404.05961)
  - additional training phase with a specially designed masked token prediction to warm-up the bidirectional attention.
  - LLM as Retrieval +2
- Wed, 8 May 2024 [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374)
  - architecture
    - v1, BertModel 22mï¼Œ	33m, 110m, 137m, 335m.  512 é•¿åº¦
    - m-long, NomicBertModel, max_trained_positions: 2048
    - m-v1.5, BertModel, 512 é•¿åº¦
  - Synthetic Data For Semantic Dense Mining
  - Tunable Hard Negative Mining
    - How hard should these negatives be for maximally effective learning in the fine-tuning phase? 
    - Our answer to this question was ultimately a tunable hard negative mining strategy in which we leveraged a preexisting text embedding model to identify and score the hardest negatives for each training example. 
    - Then, we applied a score threshold to discard the hard negatives from the above set. 
    - We found that using an upper threshold rather than a specific rank helped account for the fact that some queries admit much harder top-k negatives than others.
    - we perform a parameter sweep of the negative hardness threshold to demonstrate the value of a tunable approach (the optimal threshold value scores significantly better than other choices). 
  - Training Recipe
    - Large Scale Contrastive Pretraining With In-Batch Negatives (infoNCE)
      - Longer Truncation Length 
        - We used a document sequence length of 256 in large-scale contrastive training, in contrast to the 128 truncation length used in GTE and BGE. 
        - We truncated query sequence length to 32, consistent with BGEâ€™s source code15
    - Quality-Focused Contrastive Training With Curated Negatives
      - We truncate sequence lengths to 512 for queries and documents for all models, including the long-context variant m-long.
      -  For each query in a batch, we include one positive document and ten hard negative documents
- Sat, 11 May 2024 [Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](https://arxiv.org/abs/2405.06932)
  - å•†æ±¤çš„ piccolo2
  - Architecture bert base 24å±‚ 1024ç»´ï¼Œ 512åºåˆ—é•¿åº¦ï¼ŒMRL Training
  - æ²¡æœ‰ä»‹ç» Weakly-Supervised Contrastive Pretrainingï¼Œåª Supervised Contrastive Fine-tuningï¼Ÿ å±…ç„¶æ¯” gte-Qwen1.5-7B-instruct éƒ½å¼ºï¼Ÿï¼Ÿï¼Ÿ
  - Multi-task Hybrid Loss
    - Retrieval and Reranking Lossï¼Œuse the standard InfoNCE loss with in-batch negative
    - STS and PairClassification Lossï¼Œcosent loss function
    - Classification and Clustering Lossï¼ŒSFR embedding method
  - Datasets
    - Datasets Synthetic Pipeline
    - æ•°æ®æ”¶é›†
  - Hard Negative Mining
    - For each retrieval task, we use piccolo-base-zh [12] to conduct negative sample mining. 
    - We randomly select 15 samples from the mining negatives of rank 50 - 100 as the final hard negative samples. 
    - We avoid using higher-rank negative samples as their inclusion typically leads to a decline in performance. 
    - This is caused by a variety of reasons, such as inaccurate dataset annotation.
- Mon, 27 May 2024 [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://arxiv.org/abs/2405.17428)
  - LLM as Retrieval +3
  - Architecture
    - Mistral-7B + LLM2Vec + latent attention layer
  - Training (two-stage contrastive instruction-tuning method)
    - starting with the pretrained Mistral-7B
    - In the first stage, we apply contrastive training with instructions on retrieval datasets, utilizing in-batch negative and curated hard-negative examples
    - In the second stage, we blend carefully curated non-retrieval datasets into the stage-one training data.
  - hard-negative technique (NV-Retriever)
    - we apply the recently proposed positiveaware hard-negative technique (Moreira et al., 2024) that considers the positive relevance scores for better false negatives removal
    - Following the ablation studies in Moreira et al. (2024), we use E5-mistral-7b-instruct (Wang et al., 2023b) as a teacher retrieval model to identify the optimal hardnegative passages relevant to the query. 
    - We set the maximum threshold for negative scores based on a percentage of the positive score (TopKPercPos) with a 95% margin, described as follows: 
    - max_negative_score_threshold = pos_score * percentage_margin
  - ABLATION STUDY
    - TWO-STAGE TRAINING 
    - CAUSAL ATTENTION VS. BIDIRECTIONAL ATTENTION  
      - This indicates that embeddings generated with causal attention masks are significantly less effective than those produced with bidirectional attention masks.
    - POOLING METHODS
      - <EOS>-last, mean, latent-attention, and self-attention pooling types
      - In contrast, the latent-attention layer proved beneficial for majority of embedding tasks
    - MULTI-CLASS CLASSIFICATION AND CLUSTERING LABELS
    - HARDNEGATIVE MINING AND SYNTHETICALLY GENERATED DATASET
      - baseline (S0) 70.73
      - hard negative mining technique (S1) 71.83
      - additional public retrieval data (S2) 72.07
      - synthetically generated data (S3) 72.31
- Mon, 22 Jul 2024 [NV-Retriever: Improving text embedding models with effective hard-negative mining](https://arxiv.org/abs/2407.15831)
  - hard-negative mining
- Fri, 26 Jul 2024 [bge-multilingual-gemma2,bge-en-icl](https://github.com/FlagOpen/FlagEmbedding/tree/master)
  - 2024-07-31 MTEB enæ¦œå• 1.bge-en-icl 2.stella_en_1.5B_v5 3.SFR-Embedding-2_R 4.gte-Qwen2-7B-instruct 5.stella_en_400M_v5 6.bge-multilingual-gemma2 7.NV-Embed-v1 8.voyage-large-2-instruct 9.Linq-Embed-Mistral 10.SFR-Embedding-Mistral  
  - LLM as Retrieval +4 +5
- Mon, 29 Jul 2024 [mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval](https://arxiv.org/abs/2407.19669)
  - Alibaba çš„ mGTE
  - ä»å¤´è®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¾®è°ƒæˆä¸€ä¸ª Retrieval(Embeddings) model å’Œ Reranking Model, æœ‰é’±çœŸå¥½
  - https://huggingface.co/Alibaba-NLP/gte-multilingual-mlm-base
  - Architecture 
    - BERT + RoPE + GLU + xformersï¼Œ 12 å±‚ 768 ç»´ï¼Œ306M æ¯” bge m3 å°ï¼Œ  [CLS] pooling
    - pre-trained by masked language modeling (MLM) via a two-stage curriculum for the native 8,192 tokens context.
  - Training Recipe (three-stage training)
    - MLM pre-train (from scratch)
      - We pre-train the model via masked language modeling (MLM)ï¼ŒThe MLM probability is set to 30% 
      - To train the native 8192-context model more efficiently, we adopt a phased training curriculum (Xiong et al., 2024)
        - MLM-2048: we chunk the input into 2048 tokens and set RoPE base to 10, 000.
        - MLM-8192: we chunk the input into 8192 tokens and set RoPE base to 160, 000.
    - Retrieval(Embeddings) model
      - we construct the TRM for first-stage text retrieval in two steps: 
        - contrastive pre-training and fine-tuning (Wang et al., 2022; Li et al., 2023). 
        - Both steps share the same InfoNCE(Oord et al., 2018) learning objective
      - Contrastive Pre-Training
      - Matryoshka Embedding
      - Sparse Representation
      - Contrastive Fine-Tuning 
    - Text Reranking Model
      - It takes the query and document as input: [CLS] q [SEP] d, and directly predicts their relevance score by the [CLS] output state:
      - srerank = W h[CLS]
      - The model is fine-tuned by InfoNCE in one step6 based on our text encoder
  - reversed NTK
    - We utilize the reversed NTK scaling in contrastive pre-training to reduce required text length
    - With revNTK, models exhibit slightly lower performance on 1k context but achieve more stable 8k performance across different training steps.
  - è®ºæ–‡æ²¡æœ‰æ Hard Example Miningï¼Œä¸çŸ¥é“æ˜¯æƒ³è¡¨è¾¾ no bells and whistlesï¼Œ[Stella_v5](https://huggingface.co/NovaSearch/stella_en_1.5B_v5)ç³»åˆ—åœ¨è¿™ä¸ªåŸºç¡€ä¸Šå¾®è°ƒæ•ˆæœå°±å¥½ä¸€äº›ã€‚
  - [gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct) [gte-Qwen1.5-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct) 
    - gte-Qwen2-7B-instruct is the latest model in the gte (General Text Embedding) model family that ranks No.1 in both English and Chinese evaluations on the Massive Text Embedding Benchmark MTEB benchmark (as of June 16, 2024).
    - LLM as Retrieval +6 +7  å±…ç„¶æ²¡æœ‰å†™åœ¨è®ºæ–‡é‡Œ
- Wed, 28 Aug 2024 [Conan-embedding: General Text Embedding with More and Better Negative Samples](https://arxiv.org/abs/2408.15710)
  - dynamic hard negative mining
  - prompt-response pairs from LLMs can be used for embedding training
  - Matryoshka Embedding
- Mon, 16 Sep 2024 [jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173)
  - Architecture
    - bert_with_ropeï¼Œ MRLï¼Œ mean poolingï¼ŒFlashAttention 2ï¼ŒLoRA
    - Based on the Jina-XLM-RoBERTa architecture, this model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens.
  - Multilingual Embeddings With Task LoRA
    - retrieval.query: Used for query embeddings in asymmetric retrieval tasks
    - retrieval.passage: Used for passage embeddings in asymmetric retrieval tasks
    - separation: Used for embeddings in clustering and re-ranking applications
    - classification: Used for embeddings in classification tasks
    - text-matching: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks
  - Training
    - We initialize the model using the weights of the original XLM-RoBERTa model.
    - Pre-Trainingï¼Œ MLM whole word masking
    - Fine-Tuning for Embedding Tasksï¼Œ InfoNCE
    - Training Task-Specific Adapters
  - Failure Analysis for Asymmetric Retrieval
    - Misleading Syntactic Similarities
    - Misinterpretation of Named Entities
    - No Understanding of Polar Questions
    - Preference for Low-Quality Documents
  - Performance on LongEmbed MTEB
    - Table 5 demonstrate that jina-embeddings-v3 with the text-matching adapter achieves the highest average performance. 
    - These findings underscore the effectiveness of the RoPE-based positional embeddings, outperforming both the fixed positional embeddings used by bge-m3 and the ALiBi-based approach employed in jina-embeddings-v2.
- Fri, 28 Oct 2024 [SFR-Embedding-Mistral: Enhance Text Retrieval with Transfer Learning](https://www.salesforce.com/blog/sfr-embedding/)
  - The SFR-Embedding-Mistral marks a significant advancement in text-embedding models, building upon the solid foundations of E5-mistral-7b-instruct and Mistral-7B-v0.1
  - LoRA adapters with rank r=8 are added to all linear layers, resulting in 21M trainable parameters. 
  - Multi-task Training Benefits Generalization
    - incorporating additional clustering training yields significant improvements across all tasks
  - Task-Homogenous Batching
    - Consequently, the in-batch negative becomes more challenging as other examples within the batch closely resemble the test case scenario.
  - Impact of Hard Negatives
    - Strategy to Eliminate False Negatives
      - The results indicate that the range from 30 to 100 yields improved performance. 
      - This implies that the top-ranked documents (0-100) may include some false negatives, 
      - while those ranked lower (50-100) lack sufficient challenge.
    - Number of Hard Negatives
      - Nevertheless, our findings suggest that the training process remains relatively stable regardless of the number of hard negatives utilized.
    - Impact of Batch Size
      -  However, enlarging the batch size from 2048 to 8192 does not result in any significant change in performance.
    - Teacher models for hard negative mining
      - in general, more powerful models can yield more effective hard negatives (SFR-Embedding-Mistral > E5-Mistral > BGE-base). 
      - In the future, it will be intriguing to explore the impact of multi-round training on two fronts
  - Impact of Context Length
    - we observe that after a certain length threshold, i.e., 25 for queries and 700 for documents, 
    - BGE model is significantly less likely to rank the gold document higher than SFR-Embedding-Mistral owing to the inherent power of LLMs to represent long-context.
- Tue, 3 Dec 2024 [Arctic-Embed 2.0: Multilingual Retrieval Without Compromise](https://arxiv.org/abs/2412.04506)
  - Architecture
    - m_v2: gte-multilingual-mlm-base
    - l_v2: bge-m3-retromae
  - three-stage training framework 
    - pretraining via masked language modeling è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿˜æ˜¯æ²¡æœ‰å……åˆ†è®­ç»ƒï¼Ÿ
    - contrastive pretraining
    - Finetuning
      - Hard Negative Mining 
        - we adopt the strategy from NV Retriever
        - we confirm Moreira et al. (2024)â€™s finding that stronger teacher models yield higher-quality fine-tuning datasets 
      - Matryoshka Representation Learning
      - We also extend the maximum sequence length for queries and documents to 512 tokens, 
      - adjusting the batch size to 256 sets of 1 query, 1 positive doc, and 10 negative docs, 
      - changing the learning rate to 1e-5 and 5e-6 for medium and large models, respectively, 
      - and adjusting our WSD learning rate schedule to have no warmup and 
      - perform linear decay for 6,000 out of a total of 9,342 steps.
  - Cross-lingual Transfer
  - English Performance Gap
  - Data quality matters more than quantity.
  - Model â€œknowledgeâ€ and task-calibration are both important yet possibly orthogonal.
- Wed, 18 Dec 2024 [ModernBERT](https://arxiv.org/abs/2412.13663)
  - reduce Bias Terms, GeGLU, Rotary
  - Alternating Attention, æ¯ 3 å±‚éƒ¨ç½²å…¨å±€æ³¨æ„åŠ›, å…¶ä½™å±‚åˆ™é‡‡ç”¨128 token æ»‘åŠ¨çª—å£çš„å±€éƒ¨æ³¨æ„åŠ›
  - Model Design, æ·±è€Œçª„çš„æ¶æ„, æ¸è¿›å¼å‚æ•°ç©ºé—´æ‰©å±•
  - Training Settings: MLM use a masking rate of 30 percent, Warmup-Stable-Decay (WSD)
  - Weight Initialization and Tiling
  - è¯è¡¨è€ƒè™‘å’Œcodeæ•°æ®åŠ å…¥
- Tue, 11 Feb 2025 [Training Sparse Mixture Of Experts Text Embedding Models](https://arxiv.org/abs/2502.07972)
  - Embedding Models è¿›å…¥ Mixture Of Experts æ—¶ä»£ 
  - è®ºæ–‡è¦†ç›–å†…å®¹ä¹Ÿæ˜¯éå¸¸å…¨é¢è¯¦å®ï¼Œå®‰åˆ©
  - 3.1. Masked Language Modeling
  - 3.2. Mixture of Experts (MoE)
  - 3.3. Contrastive Learning 
    - 3.3.1. TRAINING TEXT EMBEDDING MODELS
      - Text embedding models are generally trained in two stages: weakly-supervised contrastive pretraining and contrastive finetuning
      - The contrastive pretraining stage uses the InfoNCE objective
      - Contrastive finetuning incorporates high-quality human labeled datasets and hard negatives to improve retrieval performance
      - Matryoshka Representation Learning 
    - 3.3.2. CONSISTENCY FILTERING
      - Consistency filtering improves dataset quality by removing potential false positives from weakly supervised data
    - 3.3.3. HARD NEGATIVE MINING
      - Text embedding models are typically finetuned with hard negatives mined by an existing retriever
      - positive-aware hard negative mining
- Tue, 7 Jan 2025 [voyage-3-large: the new state-of-the-art general-purpose embedding model](https://blog.voyageai.com/2025/01/07/voyage-3-large/)
  - Enabled by Matryoshka learning and quantization-aware training, 
  - voyage-3-large supports smaller dimensions and int8 and binary quantization that dramatically reduce vectorDB costs with minimal impact on retrieval quality.
  - The following figure illustrates the tradeoff between retrieval quality and storage cost (which is proportion to the number of bits per vector). 
  - We see that voyage-3-large with int8 precision and 1024 dimensions is only 0.31% below voyage-3-large with float precision and 2048 dimensions, despite using 8x less storage.
- Wed, 22 Jan 2025 Alibaba-NLP/gte-modernbert-base
- Fri, 27 Jan 2025 [ModernBERT ä¸ºæˆ‘ä»¬å¸¦æ¥äº†å“ªäº›å¯ç¤ºï¼Ÿ](https://mp.weixin.qq.com/s/RsxT7DbocGzDu_T0YnSd2Q)
  - å¯¹æ¯” ModernBERTï¼ˆ2024 å¹´ 12 æœˆï¼‰ï¼Œ jina-XLM-RoBERTaï¼ˆ2024 å¹´ 9 æœˆï¼‰ï¼Œ RoBERTa-largeï¼ˆ2019 å¹´ 7 æœˆï¼‰
  - è‹¦æ¶©çš„æ•™è®­ï¼Ÿ
    é€šè¿‡æå‡æ•°æ®åˆ©ç”¨æ•ˆç‡ï¼Œç°æœ‰è§„æ¨¡çš„åµŒå…¥æ¨¡å‹ä»æœ‰å·¨å¤§ä¼˜åŒ–ç©ºé—´ï¼Œæ ¹æœ¬æ— éœ€ç›²ç›®æ‰©å¼ è¯­æ–™åº“æˆ–å †ç Œå‚æ•°ã€‚
- Tue, 22 Apr 2025 [è…¾è®¯Conan-Embedding-V2å‘å¸ƒ](https://zhuanlan.zhihu.com/p/1897675709696149020)
  - æˆ‘ä»¬è®¾è®¡äº†Conan-1.4Bï¼ŒåŒ…å«8å±‚Attention Layersï¼ŒHidden Sizeä¸º3584ï¼Œæœ€é•¿ä¸Šä¸‹æ–‡32kã€‚å®ƒçš„å‚æ•°é‡æ˜¯1.4Bï¼Œèƒ½å¤Ÿåœ¨è¾ƒå°‘çš„å‚æ•°ä¸‹æä¾›æ›´å¤§çš„Embeddingç»´åº¦ã€‚
  - (è¿™æ˜¯ä¸€ä¸ªçŸ®èƒ–çš„æ¨¡å‹ï¼Œè·Ÿmodernbertç˜¦ç›¸å)
  - Conan-embedding-v2è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºå››ä¸ªé˜¶æ®µ
    - æˆ‘ä»¬ä»åŸºç¡€çš„å­—æ¯ã€ç¬¦å·ä¸Šï¼Œåœ¨çº¦40ä¸‡æ¡å¤šè¯­è¨€è¯­æ–™ä¸Šè®­ç»ƒäº†Conançš„BBPEåˆ†è¯å™¨ï¼Œç›®æ ‡è¯è¡¨å¤§å°15ä¸‡
    - åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒé˜¶æ®µï¼ˆç¬¬1å’Œç¬¬2é˜¶æ®µï¼‰ï¼Œæˆ‘ä»¬åŠ å…¥äº†åµŒå…¥æ•°æ®ï¼Œä»¥æ›´å¥½åœ°ä½¿LLMä¸åµŒå…¥ä»»åŠ¡å¯¹é½
    - æˆ‘ä»¬é¦–å…ˆåœ¨çº¦3T Tokensçš„é€šç”¨æ•°æ®ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œé‡ç‚¹å¢åŠ äº†é’ˆå¯¹æ€§çš„é€‚åˆåšPairçš„æ•°æ®
    - éšåï¼Œæˆ‘ä»¬æ”¶é›†äº†çº¦6äº¿æ¡ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®ï¼Œè¿™äº›æ•°æ®ä»¥é…å¯¹æ•°æ®ï¼ˆQuery - Positive Sampleï¼‰çš„å½¢å¼ç»„ç»‡ï¼Œæ ¼å¼ä¸ºæŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºã€‚
    - å¼±ç›‘ç£è®­ç»ƒ
      - åœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸LLMç›‘ç£å¾®è°ƒç›¸åŒçš„æ•°æ®ï¼Œä½†é‡‡ç”¨ä¸åŒçš„æ•°æ®æ ¼å¼å’ŒæŸå¤±å‡½æ•°ã€‚ å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æŒ‡ä»¤å’Œè¾“å…¥ä½œä¸ºæŸ¥è¯¢ï¼Œè¾“å‡ºä½œä¸ºæ­£ä¾‹æ®µè½ã€‚ 
      - ä¸ºäº†ç¡®ä¿æ›´é«˜çš„æ•°æ®è´¨é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨gte-Qwen2-7B-instructæ¨¡å‹è¿›è¡Œè¯„åˆ†ï¼Œå¹¶ä¸¢å¼ƒå¾—åˆ†ä½äº0.4çš„æ•°æ®ã€‚
      - ä¸ºäº†é«˜æ•ˆä¸”æœ‰æ•ˆåœ°åˆ©ç”¨é…å¯¹æ•°æ®ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒä¸­é‡‡ç”¨äº†InfoNCEæŸå¤±å‡½æ•°ï¼Œå¹¶ç»“åˆIn-Batch Negativeé‡‡æ ·
    - SoftMask (LLM2Vec)
      - åµŒå…¥è®­ç»ƒéœ€è¦å¯¹å¥å­è¿›è¡Œæ•´ä½“ç†è§£ï¼Œä½¿ç”¨åŒå‘æ©ç ï¼ˆbidirectional maskï¼‰è¿›è¡Œå‘é‡çº§åˆ«çš„å»ºæ¨¡ã€‚è¿™ä¸¤ç§æ©ç ä¹‹é—´å­˜åœ¨å‡ ä¸ªå…³é”®å·®è·ã€‚
      - å¦‚æœåœ¨å¼±ç›‘ç£å¾®è°ƒé˜¶æ®µç›´æ¥ä»å› æœæ©ç åˆ‡æ¢åˆ°åŒå‘æ©ç ï¼Œè®­ç»ƒå¯èƒ½ä¼šç”±äºä½ç§©è€Œå¿«é€Ÿæ”¶æ•›ï¼Œä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œä½¿å¾—è¿›ä¸€æ­¥ä¼˜åŒ–å˜å¾—å›°éš¾ã€‚
      - æˆ‘ä»¬ç»˜åˆ¶äº†ä½¿ç”¨å’Œä¸ä½¿ç”¨è½¯æ©ç æœºåˆ¶çš„æŸå¤±æ›²çº¿ã€‚ç»“æœè¡¨æ˜ï¼Œåˆå§‹é˜¶æ®µï¼Œä½¿ç”¨è½¯æ©ç çš„æŸå¤±ä¸‹é™é€Ÿåº¦æ¯”ä¸ä½¿ç”¨è½¯æ©ç çš„æŸå¤±æ›´æ…¢ã€‚
      - ç„¶è€Œï¼Œä½¿ç”¨è½¯æ©ç çš„æœ€ç»ˆæŸå¤±æ›´ä½ã€‚ è¿™è¡¨æ˜è½¯æ©ç æ–¹æ³•ä½¿æ¨¡å‹åœ¨è®­ç»ƒæ—©æœŸèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´å…¨é¢çš„ç‰¹å¾è¡¨ç¤ºã€‚
    - ç›‘ç£è®­ç»ƒ
      - æˆ‘ä»¬å°†ä»»åŠ¡åˆ†ä¸ºå››ç±»ï¼šæ£€ç´¢ã€è·¨è¯­è¨€æ£€ç´¢ã€åˆ†ç±»å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆSTSï¼‰
    - åŠ¨æ€éš¾è´Ÿä¾‹æŒ–æ˜
- Fri, 9 May 2025 [å–ä¸‹è¿™ä¸€ç¢—æ¨¡å‹æ±¤ï¼ŒæŒæ¡å‘é‡æ¨¡å‹çš„è®­ç»ƒç§˜æ–¹](https://mp.weixin.qq.com/s/BKPO_zIGvKysxmW5iMkFZg)
  - é€šè¿‡èåˆä¸åŒé˜¶æ®µçš„æ£€æŸ¥ç‚¹, æ³›åŒ–æ€§æ›´å¥½
  - ç±»ä¼¼ mergekit
- Mon, 10 Mar 2025 [Gemini Embedding: Generalizable Embeddings from Gemini](https://arxiv.org/abs/2503.07891)
  - Architecture Gemini + mean pooling, dimensional embeddings 3072 ä¼°è®¡æ˜¯ gemma-3-4b-it
  - noise-contrastive estimation (NCE) loss + MRL
  - two-stage training
    - Pre-finetuning on a large number of potentially noisy (query, target) pairs
    - Finetuning on a large mixture of task-specific datasets which contain(query, target, hard negative target) triples
  - Model Soup
    - To obtain additional generalization performance, we averaged the parameters obtained from individual fine-tuning runs
  - Improving Data Quality with Gemini
    - Synthetic Data Generation
    - Data Filtering
    - Hard Negative Mining
- Tue, 20 May 2025 [voyage-3.5 and voyage-3.5-lite: improved quality for a new retrieval frontier](https://blog.voyageai.com/2025/05/20/voyage-3-5/)
  - Matryoshka learning and quantization-aware training
- Thu, 5 Jun 2025 [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
  - The Qwen3 embedding and reranking models are built on the dense version of Qwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters
    - For text embeddings, we utilize LLMs with causal attention, appending an [EOS] token at the end of the input sequence. æ²¡æœ‰ä½¿ç”¨åŒå‘æ©ç ï¼ˆbidirectional maskï¼‰è¿›è¡Œå‘é‡çº§åˆ«çš„å»ºæ¨¡ (LLM2Vec)
    - Reranking Modelsï¼Œ ä½¿ç”¨ ç”Ÿæˆæ¨¡å‹çš„æ¨¡ç‰ˆï¼ŒReranking è®¤ä¸ºæ˜¯ä¸ªç”Ÿæˆ yes å’Œ no tokené—®é¢˜
  - Models Training
    - Embedding Models
      - Training Objective InfoNCE
      - stage 1 Large-Scale Synthetic Data-Driven Weak Supervision Training
        - Large-Scale Synthetic Data-Driven Weak Supervision Training
        - Due to the exceptional performance of the Qwen3 Foundation model, the synthesized data is of notably high quality.\
      - stage 2 Supervised Fine-Tuning with High-Quality Synthetic and labeled Data
      - stage 3 Model Merging using sampled Checkpoints from stage 2
    - Reranking Models
      - Supervised Fine-Tuning (SFT) loss
      - stage 2 Supervised Fine-Tuning with High-Quality Synthetic and labeled Data
      - stage 3 Model Merging using sampled Checkpoints from stage 2
  - Analysis
    - Effectiveness of Large-Scale Weakly Supervised Pre-Training
    - Effectiveness of Model Merging
    - æ²¡æœ‰ä½¿ç”¨ LLM2Vec, æ²¡æœ‰ä½¿ç”¨ Hard Negative Mining, æ²¡æœ‰ç”¨ Knowledge distillationï¼Œ æ²¡æœ‰å¤šå„ç§ä»»åŠ¡ä½¿ç”¨ä¸åŒçš„ instruct
    - Without bells and whistles çš„æ„Ÿè§‰
- Thu, 4 Sep 2025 [EmbeddingGemma3](https://huggingface.co/blog/embeddinggemma)
  - https://huggingface.co/collections/google/embeddinggemma-68b9ae3a72a82f0562a80dc4
  - Architecture
    - Gemma3 308M
    - use bi-directional attention instead of causal (one-way) attention
    - 2kâ€‘token context window
    - mean pooling 
    - two dense layers transform the text embeddings into their final form, a 768-dimensional vector.
    - MRL  512, 256, or 128
    - approximately 320 billion tokens

# Retrieval(Embeddings) benchmark
- Sat, 17 Apr 2021 [BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models](https://arxiv.org/abs/2104.08663)
  - 18ä¸ªæ•°æ®é›†
  - average query length between 3 and 192 words
  - average document length between 11 and 635 words
- Mon, 12 Feb 2024 [Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT](https://arxiv.org/abs/2402.07440)
  - 12 task 
- Thu, 13 Oct 2022 [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316)
  - Hugging Face ä¸»å¯¼çš„ Massive Text Embedding Benchmark
  - https://huggingface.co/spaces/mteb/leaderboard
- Fri, 7 Apr 2023 [T2Ranking: A large-scale Chinese Benchmark for Passage Ranking](https://arxiv.org/abs/2304.03679)
  - åˆ¶ä½œäº†ä¸€ä¸ª T2Ranking æ•°æ®é›†
  - Chinese BERTbase è®­ç»ƒäº† retrieval å’Œ Re-rank
- Thu, 18 Apr 2024 [LongEmbed: Extending Embedding Models for Long Context Retrieval](https://arxiv.org/abs/2404.12096)
   - æ£€ç´¢æ¨¡å‹è¿›å…¥é•¿ä¸Šä¸‹æ–‡æ—¶ä»£ï¼ŒRoPE å«é‡‘é‡è¿˜åœ¨ä¸æ–­ä¸Šå‡
   - LONGEMBED benchmark, which includes two synthetic and four real-world tasks
   - we pretrain E5-RoPE following the training procedure and data of E5.
   - E5Base and E5-RoPEBase are selected as the comparison subjects thanks to their shared training process, training data, and comparable performance on BEIR and LONGEMBED benchmarks.
   - Comparison of Extension Methods
     - APE-based Models. 
       - We observe that plug-and-play methods including GP, RP, PI and PCW strategies yield comparable results with no significant disparities.
       - On the other hand, further tuning consistently yields additional performance gains for both models, across all target context lengths.
       - This suggests that freezing the original model weights and fine-tuning exclusively the added position embeddings can effectively extend the modelâ€™s context window while strictly maintaining modelâ€™s original ability
     - RoPE-based Models.
       - It is observed that RoPE-specific methods including NTK and SE yield significant improvements for both models across all datasets, surpassing PCW, PI and GP by a large margin.
       - SE / NTK is short for SelfExtend / NTK-Aware Interpolation
     - Further, our analysis reveals the superiority of RoPE-based embedding models over APE-based ones in context window extension. 
     - Hence, we advocate for the use of RoPE for future embedding models.
- Tue, 16 Jul 2024 [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2407.12883)
  - we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. 
  - æˆ‘è§‰å¾—ä½ åœ¨æ•…æ„éš¾ä¸ºæ£€ç´¢æ¨¡å‹
  - MAIN RESULTS
    - Existing retrieval systems perform poorly on BRIGHT.
    - Querying with LLM reasoning steps improves retrieval performance.
    - Retrieval augmentation boosts performance in question-answering. 
  - ANALYSIS
    - RERANKING WITH LLMS ENHANCES RETRIEVAL PERFORMANCE
    - ROBUSTNESS AGAINST DATA LEAKAGE FROM PRETRAINING  
    - LONG-CONTEXT RETRIEVAL WITH A REDUCED SEARCH SPACE IS CHALLENGING
- Tue, 17 Dec 2024 [AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark](https://arxiv.org/abs/2412.13102)
  - Automated, Heterogeneous, Dynamic
  - Candidate Generation
    - Sample one document from the raw corpus as the positive document
    - Prompt LLM to generate the characters who might find the document useful
    - Prompt LLM to generate the scenarios in which the character might find the document useful
    - Prompt LLM to generate the query ori_qi based on the specific character and scenario
    - To diversify the generated queries, we consider the following attributes when designing the prompt: query length, query type, information-based type, and expression style.
    - Prompt LLM to rewrite the generated query for multiple times to try to avoid the duplicated tokens as in the raw corpus
    - Prompt LLM to generate some hard negative documents based on the generated query qi and the positive document
    - Repeat Step 1-6
  - Quality Control
    - Filter low-quality queries
      - To improve the quality of generated queries, we utilize LLM to access the relevance between the query qi and the positive document
    - Correct the false relevance labels 
      - we design a three-step pipeline to correct the false relevance labels
      - Recall with embedding model
      - Pre-label with re-ranking models
        - Use multiple re-ranking models to re-rank Lrecall
      - Label with LLM (Use LLM as labeler.)
- Wed, 19 Feb 2025 [MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.13595)
  - we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. 
  - These optimizations allow us to introduce benchmarks that drastically reduce computational demands

# æ€»ç»“ï¼Œå¦‚ä½•è®­ç»ƒä¸€ä¸ªæ•ˆæœå¾ˆå¥½çš„Retrieval(Embeddings) model
- æ¨¡å‹
  - Dense Retrieval æ€»ä½“å±•ç°å‡ºæ˜æ˜¾çš„ Scaling Lawsï¼Œä½†æ—¶ä¸æ—¶ä¹Ÿæœ‰å°æ¨¡å‹çš„åœ¨MTEBæ¦œå•å‰åˆ—
    - å•è¯­è¨€å¯¹æ ‡ BERT BASEï¼šL=12ï¼ŒH=768ï¼ŒA=12 110M
    - å¤šè¯­è¨€å¯¹æ ‡ BERT LARGEï¼šL=24ï¼ŒH=1024ï¼ŒA=16 340M
    - æ›´å¤§çš„æ¨¡å‹æ¯”å¦‚ 1Bï¼Œ7B å¯¹äº Embeddings æ¥è¯´çœŸçš„æœ‰æ„ä¹‰å—å­˜ç–‘
  - é€‰æ‹©é€‚åˆçš„åŸºç¡€æ¨¡å‹ï¼Œå¤šè¯­è¨€èƒ½åŠ›å’Œé•¿ä¸Šä¸‹æ–‡èƒ½åŠ›æ¯”è¾ƒé‡è¦ï¼Œä½† bert éƒ½æ˜¯2020å¹´å·¦å³è®­ç»ƒçš„ï¼Œæ™®éä¸å¦‚ç°åœ¨llmè®­ç»ƒçš„å……åˆ†
  - è¶Šæ¥è¶Šå¤šçš„ Large decoder-only language models (LLMs) as Retrievalçš„æ¨¡å‹ä¸ŠMTEBæ¦œï¼ŒåŸºç¡€æ¨¡å‹é€‰æ‹©èŒƒå›´å°±å¤§å¤§æ‹“å®½äº†
  - æ›´æœ‰é’±çš„å…¬å¸ä¼šä»å¤´è®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå¾®è°ƒæˆä¸€ä¸ªæ£€ç´¢æ¨¡å‹ æ¯”å¦‚ ST5ã€mGTEã€‚ã€‚
- æ•°æ®
  - ä½¿ç”¨ GPT åˆæˆæ•°æ®
  - Weakly-Supervised Contrastive Pre-training + Supervised Fine-tuning
  - çŸ¥è¯†è’¸é¦
- ç®—æ³•
  - CONSISTENCY FILTERING + é€‚åˆéš¾åº¦çš„ Hard Example Mining
  - ä»å…¶ä»– Metric Learning å’Œ Contrastive Learning å­¦ä¹ ä»»åŠ¡ä¸­å¯»æ‰¾å¯å‘
  - å¤šä»»åŠ¡å­¦ä¹ ï¼Œæ¨¡å‹è’¸é¦

# Rerank model
å¾ˆå¤šæ–‡ç« å§ Rerank model ç§°ä¸º cross-encoderï¼Œç›¸å¯¹ä¸ Dense Retrieval çš„ dual-encoderã€‚
ç›¸æ¯” Dense Retrieval æ¨¡å‹ï¼Œç®—æ³•ä¸Šå¯ä»¥ç»“åˆMetric Learning å’Œ Contrastive Learningï¼Œç³»ç»Ÿä¸Šå¯ä»¥è·Ÿ approximate nearest neighbor ç»“åˆï¼Œ ä¸‹æ¸¸ä»»åŠ¡åˆå¯ä»¥è·Ÿ  large-scale open-domain QA ç»“åˆã€‚
Rerank model çœŸçš„è¦æ— èŠå¾ˆå¤šï¼ŒRerank model æœ¬è´¨ä¸Šå°±æ˜¯ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼Œçº¦ç­‰äº bert é¢„è®­ç»ƒä»»åŠ¡ Next Sentence Predictionã€‚
- Sun, 13 Jan 2019 [Passage Re-ranking with BERT](https://arxiv.org/abs/1901.04085)
  - Re-ranking çš„å†å²æ¯” Dense Retrieval è¿˜æ—©
- Thu, 12 Oct 2023 [Fine-Tuning LLaMA for Multi-Stage Text Retrieval](https://arxiv.org/abs/2310.08319)
  - llm as Reranker
- Sun, 4 Feb 2024 [ä¸ºRAGè€Œç”Ÿ-BCE embeddingæŠ€æœ¯æŠ¥å‘Š](https://zhuanlan.zhihu.com/p/681370855)
  - æˆ‘ä»¬å°†BCEmbeddingè®¾è®¡ä¸ºäºŒé˜¶æ®µæ£€ç´¢å™¨ï¼Œåˆ†å·¥åˆä½œï¼šâ€œç¦»çº¿â€çš„Embeddingè´Ÿè´£å°½å¯èƒ½å¬å›ï¼Œâ€œåœ¨çº¿â€çš„Rerankerè´Ÿè´£ç²¾æ’å’Œä½è´¨é‡è¿‡æ»¤ã€‚
    - ç²¾æ’é˜¶æ®µä¸ºäº†è§£å†³ä¿¡æ¯äº¤äº’çš„é—®é¢˜ï¼Œé‡‡ç”¨cross-encoderæ¶æ„ï¼ˆå¦‚å›¾äºŒ-2 (b)æ‰€ç¤ºï¼‰ã€‚Rerankeræ¨¡å‹å¯ä»¥å®ç°ç”¨æˆ·é—®é¢˜å’ŒçŸ¥è¯†åº“è¯­æ–™çš„ä¿¡æ¯äº¤äº’ï¼Œä½¿æ¨¡å‹å¯ä»¥â€œè§æœºè¡Œäº‹â€åœ°è¯†åˆ«åˆ°æ›´åŠ å‡†ç¡®çš„è¯­ä¹‰å…³ç³»ï¼Œç®—æ³•æ€§èƒ½ä¸Šé™å¯ä»¥å¾ˆé«˜ã€‚è¯¥æ–¹å¼çš„ç¼ºç‚¹æ˜¯ï¼Œéœ€è¦å¯¹ç”¨æˆ·é—®é¢˜å’ŒçŸ¥è¯†åº“è¯­æ–™è¿›è¡Œåœ¨çº¿ï¼ˆonlineï¼‰åœ°è¯­ä¹‰å…³ç³»æå–ï¼Œæ•ˆç‡æ¯”è¾ƒä½ï¼Œæ— æ³•å¯¹å…¨é‡çš„çŸ¥è¯†åº“è¯­æ–™è¿›è¡Œå®æ—¶å¤„ç†ã€‚
    - ç»“åˆå¬å›å’Œç²¾æ’äºŒè€…çš„ä¼˜åŠ¿ï¼Œå¬å›é˜¶æ®µå¯ä»¥å¿«é€Ÿæ‰¾åˆ°ç”¨æˆ·é—®é¢˜ç›¸å…³æ–‡æœ¬ç‰‡æ®µï¼Œç²¾æ’é˜¶æ®µå¯ä»¥å°†æ­£ç¡®ç›¸å…³ç‰‡æ®µå°½å¯èƒ½æ’åœ¨é å‰ä½ç½®ï¼Œå¹¶è¿‡æ»¤æ‰ä½è´¨é‡çš„ç‰‡æ®µã€‚äºŒé˜¶æ®µæ£€ç´¢å¯ä»¥å¾ˆå¥½åœ°æƒè¡¡æ£€ç´¢æ•ˆæœå’Œæ•ˆç‡ï¼Œå…·æœ‰å·¨å¤§åº”ç”¨ä»·å€¼ã€‚
  - æœ‰æ„ä¹‰çš„Rerankåˆ†æ•°
    - â€œè¯„åˆ¤æ ‡å‡†â€ä¸­å¥½æ£€ç´¢å™¨è¿˜æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼Œå¯ä»¥è¿‡æ»¤ä½è´¨é‡ä¿¡æ¯ã€‚æˆ‘ä»¬è®¾è®¡çš„Rerankeræ¨¡å‹ï¼Œè¾“å‡ºçš„(query, passage)è¯­ä¹‰ç›¸å…³åˆ†æ•°ï¼Œä¸ä»…èƒ½ç”¨æ¥åšpsaagesæ’åºï¼Œå…¶åˆ†æ•°çš„ç»å¯¹å€¼å¯è¡¨å¾çœŸå®çš„è¯­ä¹‰ç›¸å…³ç¨‹åº¦ï¼Œè¿™å¯ä»¥ç”¨æ¥åˆ¤æ–­å“ªäº›æ˜¯ä½è´¨é‡passagesï¼Œå®ç°ä½è´¨é‡ç‰‡æ®µè¿‡æ»¤ã€‚è¿™å¯¹RAGä¸­LLMå›ç­”é—®é¢˜éå¸¸æœ‰å¸®åŠ©ï¼Œæ›´å¹²ç»ƒã€å¹²æ‰°ä¿¡æ¯å°‘çš„contextï¼Œå¯ä»¥æœ‰æ•ˆæé«˜LLMå›ç­”è´¨é‡[17]ã€‚
    - æ ¹æ®æˆ‘ä»¬ä¸šåŠ¡å®è·µç»éªŒå’Œå¼€æºç¤¾åŒºçš„åé¦ˆï¼Œbce-reranker-base_v1è¾“å‡ºçš„åˆ†æ•°æ¨èä»¥0.35ï½0.4ä¸ºé˜ˆå€¼ï¼Œæ¥è¿›è¡Œä½è´¨é‡passageè¿‡æ»¤ã€‚ç”¨æˆ·å®é™…ä½¿ç”¨åé¦ˆï¼Œæ”¶è·å¾ˆä¸é”™çš„æ•ˆæœã€‚
- Mon, 18 Mar 2024 [bge-reranker-v2-m3ã€BAAI/bge-reranker-v2-gemmaã€ BAAI/bge-reranker-v2-minicpm-layerwise.](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker)
  - reranker éƒ½ä¸é…æœ‰ç¯‡æŠ€æœ¯æŠ¥å‘Š
  - LLM-based reranker
- Tue, 25 Jun 2024 [Jina Reranker v2 for Agentic RAG: Ultra-Fast, Multilingual, Function-Calling & Code Search](https://jina.ai/news/jina-reranker-v2-for-agentic-rag-ultra-fast-multilingual-function-calling-and-code-search)
  - Jina Reranker v2 å°† Reranker ç©å‡ºäº†æ–°é«˜åº¦
  - Multilingual: More relevant search results in 100+ languages, outperforming bge-reranker-v2-m3;
  - Agentic: State-of-the-art function-calling and text-to-SQL aware document reranking for agentic RAG;
  - Code retrieval: Top performance on code retrieval tasks, and
  - Ultra-fast: 15x more documents throughput than bge-reranker-v2-m3, and 6x more than jina-reranker-v1-base-en.
- Fri, 26 Jul 2024 [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight)
  - trained based on gemma2-9b
- Mon, 29 Jul 2024 [mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval](https://arxiv.org/abs/2407.19669)
  - ä»å¤´è®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼Œå¹¶å¾®è°ƒæˆä¸€ä¸ª Retrieval(Embeddings) model å’Œ Reranking Model, æœ‰é’±çœŸå¥½
- Thu, 7 Nov 2024 [Best Practices for Distilling Large Language Models into BERT for Web Search Ranking](https://arxiv.org/abs/2411.04539)
- Wed, 22 Jan 2025 [Alibaba-NLP/gte-reranker-modernbert-base](https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base)
- Wed, 13 Mar 2025 [Baked-in Brilliance: Reranking Meets RL with mxbai-rerank-v2](https://www.mixedbread.com/blog/mxbai-rerank-v2)
  - Qwen2ForCausalLM
  - we used a three-step reinforcement-learning process:
    - GRPO (Guided Reinforcement Prompt Optimization)
    - Contrastive Learning
    - Preference Learning
- Wed, 4 Jun 2025 [ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking](https://www.arxiv.org/abs/2506.03487)
  - Qwen2.5 llm as reranker
  - In summary, our paper demonstrates three-fold contributions:
    - Our quantitative analysis reveals that SLMs struggle with understanding task prompts and generating correctly formatted outputs for reranking tasks without task-specific fine-tuning.
    - We propose a novel two-stage approach, ProRank, to activate the power of SLMs to effectively rerank documents with interpretable relevance scores, combining GRPO (Shao et al., 2024) for coarse-grained scoring, followed by fine-grained scoring.
    - Extensive evaluations demonstrate that our approach achieves superior reranking performance, with ProRank 0.5B SLM model outperforming the larger 32B fine-tuned LLM reranking models.
  - GRPO å¯¹è¾“å‡ºæ ¼å¼æ˜¯å¦ä¸º 0ï¼Œ1 æœ‰æ˜æ˜¾å¸®åŠ©ã€‚ ä½†GRPO + sft ç›¸æ¯” ç›´æ¥sftæ˜¯å¦æœ‰æé«˜å¥½åƒæ²¡æœ‰è¯´æ¸…æ¥šï¼Œä¸å¤±ä¸ºä¸€ç§ Warmup çš„æ–¹æ³•ã€‚
- Thu, 5 Jun 2025 [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
  - Reranking Modelsï¼Œ ä½¿ç”¨ ç”Ÿæˆæ¨¡å‹çš„æ¨¡ç‰ˆï¼ŒReranking è®¤ä¸ºæ˜¯ä¸ªç”Ÿæˆ yes å’Œ no tokené—®é¢˜
  - Models Training
    - Reranking Models
      - Supervised Fine-Tuning (SFT) loss
      - stage 2 Supervised Fine-Tuning with High-Quality Synthetic and labeled Data
      - stage 3 Model Merging using sampled Checkpoints from stage 2
- Fri, 22 Aug 2025 [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
  - llm + pairwise & listwise approaches æœ‰æ²¡æœ‰æå¤´ 
  - colbert ranker ç¡®å®æ¯” embeddingå¼ºä¸€äº›ï¼Œæ¯”æ­£ç»çš„rankerå¼±ä¸€äº›

# listwise reranker
- Fri, 22 Aug 2025 [How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models](https://arxiv.org/abs/2508.16757)
  - llm + pairwise & listwise approaches æœ‰æ²¡æœ‰æå¤´
- Mon, 29 Sep 2025 [jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking](https://arxiv.org/abs/2509.25085)
  - Built upon Qwen3-0.6B
  - listwise reranker that introduces a novel last but not late interaction
    - Prompt Template: Query Document 1 <|doc_emb|> Document 2 <|doc_emb|> Document 3 <|doc_emb|> Query <|query_emb|> 
    - <|doc_emb|> & <|query_emb|> -> projector -> cosine score
  - Training
    - Loss Functions: InfoNCE + dispersive loss + similarity loss
    - Multi-Stage Training
      - Stage 1: Foundation Specialization 
      - Stage 2: Context and Hard Negative Mining
        - Training systematically mines hard negatives across multiple retrieval systems including BGE, Jina, GTE, and E5-Large with up to 25 negatives per query
      - Stage 3: Model Ensemble and Optimization

# ColBERT
ColBERT + Late Chunking æœ‰æ²¡æœ‰æå¤´ï¼Ÿ
- Mon, 27 Apr 2020 [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832)
- Thu, 2 Dec 2021 [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488)
- Tue, 23 Apr 2024 [A Reproducibility Study of PLAID](https://arxiv.org/abs/2404.14989)
  - maxsim åŠ é€Ÿ
- Wed, 29 May 2024 [MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings](https://arxiv.org/abs/2405.19504)
  - maxsim åŠ é€Ÿ
- Mon, 23 Sep 2024 [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683)
  - Token Pooling ä¸€åŠï¼Œæ€§èƒ½ç”šè‡³æœ‰æ‰€æå‡
- Thu, 10 Jul 2025 [maxsim-cpu](https://github.com/mixedbread-ai/maxsim-cpu)
  - maxsim åŠ é€Ÿ

# Sparse Retrieval
- Mon, 12 Jul 2021 [SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking](https://arxiv.org/abs/2107.05720)
- Tue, 21 Sep 2021 [SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval](https://arxiv.org/abs/2109.10086)
- Mon, 11 Mar 2024 [SPLADE-v3: New baselines for SPLADE](https://arxiv.org/abs/2403.06789)

# reasoning-intensive retrieval
- Tue, 16 Jul 2024 [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2407.12883)
  - we introduce BRIGHT, the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents. 
- Tue, 29 Apr 2025 [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
  - Pilot Study
    - Existing public training datasets are helpful for factual retrieval but not for reasoning-intensive retrieval
      - Queries from these two datasets are mostly simple factual questions, whose relevant documents can often be retrieved using direct lexical or semantic matching. 
      - However, queries in reasoning benchmarks are much longer and more complex. 
    - Longer effective context length is desirable to better leverage test-time scaling through query rewriting.
      - the length of a rewritten query can be a new dimension of test-time scaling and a longer effective context length is desirable for long rewritten queries.
      - Query decomposition has been shown to be effective in multi-hop retrieval tasks
        - an information-rich long query is better than several decomposed short queries on BRIGHT
  - ReasonIR: Synthesizing Hard and Varied-length Retriever Training Data
    - public data to specifically train a general autoregressive LLM for retrieval
    - varied-length (VL ) data to extend the effective context length of the retriever for input queries
      - we ask the LLM to also generate a positive document for the query, following the distillation idea in Wang et al. (2023b)
    - hard query (HQ) data to improve the retrieverâ€™s ability to handle reasoning-intensive queries
      - we synthesize reasoning-intensive training data by generating hard queries (HQ) from high-quality documents using a â€œhuman-like brainstorm guidelineâ€ for hard query generation.
      - Reasoning-worthy seed document selection
      - Reasoning-intensive document-to-query generation.
        - An ideal set of reasoning-intensive queries has three properties: challenging, self-contained, diverse
        - As previous work has shown unsuccessful attempts on directly prompting an LLM to generate difficult questions
        - we ask the LLM to reason about the background knowledge, common problem-solving patterns, and realistic scenarios before formulating a difficult question.
        - ...
    - Multi-turn Hard Negative Generation
      - Existing research typically identifies hard negatives by selecting top-ranked but irrelevant documents from a retriever such as BM25
      - However, we find that this does not work for reasoning-intensive queries for 3 reasons:
        - First, existing retrievers perform poorly on reasoning-intensive queries
        - Second, the goal of retrieval has shifted from finding documents that contain direct answers to finding a wide range of documents that are helpful for reasoning
        - Third, the seed document may not be the most relevant to the generated query
      - generating the hard negative in a separate turn
    - Reasoning-intensive Information Retrieval (IR) Performance
      - REASONIR-8B benefits from test-time scaling with query rewriting on BRIGHT
      - REASONIR-8B can form an ensemble with a sparse retriever or be combined with an LLM-based reranker for better retrieval.
- Thu, 22 May 2025 [lightonai/Reason-ModernColBERT](https://huggingface.co/lightonai/Reason-ModernColBERT/tree/main)
  - Reason-ModernColBERT is a late interaction model trained on the reasonir-hq dataset. 
  - It achieves extremely competitive performance on the BRIGHT benchmark aimed at evaluating reasoning-intensive retrieval performance
  - modernbert å’Œ ColBERT çš„å«é‡‘é‡ä¸æ–­æå‡
- Mon, 11 Aug 2025 [DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval](https://arxiv.org/abs/2508.07995)
  - DIVER-DChunk
    -  To handle lengthy documents, we employed the Chonkie2 library to perform semantic-aware chunking. 
    - Using the Qwen3-Embedding-0.6B (Zhang et al., 2025b) model with a similarity threshold of 0.5, 
    - the text was divided into smaller chunks of up to 4096 tokens, with a minimum size of one sentence per chunk. 
  - DIVER-QExpand
    - we retain its iterative design in DIVER-QExpand but make two practical modifications
      - First, we replace the BM25 retriever with a dense retriever trained for reasoningintensive task
      - Second, instead of concatenating all intermediate query expansions, which can often exceed 2000 tokens, we simplify the process by retaining only the original query and the final-round expansion. 
  - DIVER-Retriever
    - é€šç”¨Retrieverä¸å¦‚Reasonä¸“ç”¨Retrieverï¼ŒQwen3-4B Avgåªæœ‰ 5.6 2333
    - +BM25 (Hybrid) æé«˜æ¯”è¾ƒå¤§
  - DIVER-Rerank
    - DIVER(v2) includes advanced query expansion and combined pointwise and listwise DIVER-Rerank, achieving the latest state-of-the-art.
    - DIVER(v2) reaches an nDCG@10 of 45.8, surpassing BGE-Reasoner by +0.8 points and establishing a new SOTA
    - To complement this local evaluation, the listwise module (DIVER-Rerank-Listwise) employs an LLM (e.g.
Deepseek-R1-0528) to directly rank the top-100 candidate documents of the query, offering a global view
of document relevance. The final reranking result integrates both modules, leveraging the fine-grained
local scoring of pointwise rerankers and the holistic ranking ability of listwise rerankers.
    - ä½¿ç”¨ Deepseek-R1-0528 rerank top-100 æ˜¯å¦æ€æ­»äº†æ¯”èµ›


# learned sparse representations \ late-interaction methods
- Mon, 27 Apr 2020 [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832)
- Mon, 5 Feb 2024 [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216)
- Mon, 23 Sep 2024 [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling](https://arxiv.org/abs/2409.14683)

# Chunking / Chucking Granularity
[è·³è½¬](./awesome_rag.md#chunking--chucking-granularity)

# Toolkit 
- Tue, 27 Aug 2019 [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
  - [sentence-transformers](https://github.com/UKPLab/sentence-transformers/)
  - [Document](https://www.sbert.net/)
- Fri, 19 Feb 2021 [Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations](https://arxiv.org/abs/2102.10073)
  - [pyserini](https://github.com/castorini/pyserini)

# Other
- Fri, 23 Feb 2024 [Self-Retrieval: Building an Information Retrieval System with One Large Language Model](https://arxiv.org/abs/2403.00801)
  - LLM can memorize (passage -> title)
  - è‡³å°‘Hit@1ã€ Hit@5ã€ MRR@5 æŒ‡æ ‡æ¯” dense retrieval æ¨¡å‹ GTR BGE OpenAI æ•ˆæœå¥½?? æ‰€ä»¥ dense retrieval å¿…é¡»é…åˆ reranker ??

# Architecture
- Tue, 20 Apr 2021 [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
  - Rotary Position Embedding
- Wed, 7 Dec 2022 [Text Embeddings by Weakly-Supervised Contrastive Pre-training](https://arxiv.org/abs/2212.03533)
  - ç»å…¸ Bert 512 é•¿åº¦
- Mon, 30 Oct 2023 [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents](https://arxiv.org/abs/2310.19923)
  - BERT with ALiBi, GEGLU, BF16, mean pooling
- Fri, 29 Dec 2023 [MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining](https://arxiv.org/abs/2312.17482)
  - This architecture combines FlashAttention [11], ALiBi [44], Gated Linear Units[12, 50], a dynamic unpadding module [66], and low precision LayerNorm.
- Fri, 2 Feb 2024 [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://arxiv.org/abs/2402.01613)
  - NomicBertModel æ¶æ„æ¯”è¾ƒç°ä»£ bert_with_rope
  - rotary + SwiGLU + Flash Attentionï¼Œ12å±‚768ç»´ï¼Œ137Bï¼Œè®­ç»ƒé•¿åº¦2048
- Mon, 5 Feb 2024 [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216)
  - ä½¿ç”¨ XLM-RoBERTaï¼Œ ç»å¯¹ä½ç½®ç¼–ç ï¼Œ8192 é•¿åº¦ï¼Œ23 å±‚ 1024ç»´
- Wed, 8 May 2024 [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374)
  - v1, BertModel 22m, 33m, 110m, 137m, 335m.  512 é•¿åº¦
  - m-long, NomicBertModel, max_trained_positions: 2048
  - m-v1.5, BertModel, 512 é•¿åº¦
- Mon, 27 May 2024 [NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models](https://arxiv.org/abs/2405.17428)
  - Mistral-7B + LLM2Vec + latent attention layer
- Mon, 29 Jul 2024 [mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval](https://arxiv.org/abs/2407.19669)
  - BERT + RoPE + GLU + xformersï¼Œ 12 å±‚ 768 ç»´ï¼Œ306M æ¯” bge m3 å°
  - pre-trained by masked language modeling (MLM) via a two-stage curriculum for the native 8,192 tokens context.
- Tue, 3 Dec 2024 [Arctic-Embed 2.0: Multilingual Retrieval Without Compromise](https://arxiv.org/abs/2412.04506)
  - m_v2: gte-multilingual-mlm-base
  - l_v2: bge-m3-retromae
- Mon, 16 Sep 2024 [jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173)
  - Based on the Jina-XLM-RoBERTa architecture, this model supports Rotary Position Embeddings to handle long input sequences up to 8192 tokens.
- Tue, 3 Dec 2024 [Arctic-Embed 2.0: Multilingual Retrieval Without Compromise](https://arxiv.org/abs/2412.04506)
  - m_v2: gte-multilingual-mlm-base
  - l_v2: bge-m3-retromae
- Wed, 18 Dec 2024 [ModernBERT](https://arxiv.org/abs/2412.13663)
- Tue, 11 Feb 2025 [Training Sparse Mixture Of Experts Text Embedding Models](https://arxiv.org/abs/2502.07972)
  - Embedding Models è¿›å…¥ Mixture Of Experts æ—¶ä»£
- Thu, 5 Jun 2025 [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
  - Qwen3

# Training data
## Synthetic data (Data Augmentation (DA))
- Tue, 17 Dec 2024 [AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark](https://arxiv.org/abs/2412.13102)
  - Automated, Heterogeneous, Dynamic
- Thu, 5 Jun 2025 [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
  - stage 2 Supervised Fine-Tuning with High-Quality Synthetic and labeled Data
### Synthetic Query (Query Augmentation, pseudo query generation(GenQ))
- Wed, 17 Apr 2019 [Document Expansion by Query Prediction](https://arxiv.org/abs/1904.08375)
  - the task is to predict a set of queries for which that document will be relevant.
  - We optionally re-rank these retrieved documents using BERT (Devlin et al., 2018) as described by Nogueira and Cho (2019).
- Dec, 2 2019 [From doc2query to docTTTTTquery](https://cs.uwaterloo.ca/~jimmylin/publications/Nogueira_Lin_2019_docTTTTTquery-v2.pdf)
  - å¥½çŸ­ï¼Œåªæœ‰ä¸‰é¡µ
  - ç”¨ T5 åš Query ç”Ÿæˆï¼Œ æå¦‚éš”ä¸–ï¼Œäººç±»èƒ½ç”Ÿæˆè‡ªç”±åªæœ‰çŸ­çŸ­å‡ å¹´
- Wed, 29 Apr 2020 [Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation](https://arxiv.org/abs/2004.14503)
  - QGen (3 Synthetic Question Generation)
  - Our question generator is an encoder-decoder with Transformer
  - Parameter weights are also shared and are initialized from a pretrained RoBERTa (Liu et al., 2019) checkpoints.
- Mar 1, 2021[BeIR/query-gen-msmarco-t5-large-v1](https://sbert.net/examples/sentence_transformer/unsupervised_learning/query_generation/README.html)
  - è¿˜åœ¨ä½¿ç”¨docTTTTTquery
- Tue, 14 Dec 2021 [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://arxiv.org/abs/2112.07577)
  Query Generation via T5 (DocT5Query)
- 2022.11.30. OpenAI å‘å¸ƒGPT-3.5
- Wed, 15 Feb 2023 [How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval](https://arxiv.org/abs/2302.07452)
  - ä½¿ç”¨ docTTTTTqueryï¼Ÿä¸ºä»€ä¹ˆä¸ç”¨GPT-3.5
- Sat, 24 Feb 2024 [OpenAI vs Open-Source Multilingual Embedding Models Choosing the model that works best for your data](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05)
  - ç”¨ChatGPTåˆæˆé—®ç­”æ•°æ®é›†è®­ç»ƒæ¨¡å‹ï¼Œç”¨ChatGPTåˆæˆé—®ç­”æ•°æ®é›†æµ‹è¯•æ¨¡å‹çš„ä¸–ç•Œè¾¾æˆäº†
- Mon, 5 Feb 2024 [BGE M3-Embedding](https://arxiv.org/abs/2402.03216)
  - continue pretraining (RetroMAE) 
    - We can observe that RetroMAE can significantly improve the retrieval performance, and pre-training on unsupervised data can further enhance the retrieval quality of the embedding model.
  - Supervised Fine-tuning https://huggingface.co/datasets/Shitao/bge-m3-data
    - 8è‹±æ–‡7ä¸­æ–‡2å…¶ä»–ï¼Œæ··åˆæ•°æ®
  - synthetic data ä½¿ç”¨ GPT3.5 åšæ•°æ®åˆæˆ https://huggingface.co/datasets/Shitao/MLDR
    - MLDR is a Multilingual Long-Document Retrieval dataset built on Wikipeida, Wudao and mC4, covering 13 typologically diverse languages. Specifically, we sample lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. 
- Wed, 8 May 2024 [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374)
  - we leverage Large Language Models to generate novel queries
- Fri, 29 Mar 2024 [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327)
  - LLM-based Diverse Query Generation
    - we employ few-shot prompts to control the diversity of queries
### Synthetic Document
- Thu, 20 Jul 2023 [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models]
  - This dataset, based on positive pairs from the SNLI dataset and negatives created with GPT-3.5
- Fri, 22 Sep 2023 [AnglE-optimized Text Embeddings](https://arxiv.org/abs/2309.12871)
  - ä¸º stsä»»åŠ¡ ç”Ÿæˆ positive/negative pairs
- Sun, 31 Dec 2023 [Improving Text Embeddings with Large Language Models](https://arxiv.org/abs/2401.00368)
  - query positive_document hard_negative_document å…¨åˆæˆå•Šï¼Ÿï¼Ÿè¿™ä¹Ÿå¤ªé‡äº†
### LLM Scoring
- Mon, 30 Jan 2023 [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652)
  - REPLUG LSR (REPLUG with LM-Supervised Retrieval) treating the LM as a frozen, black-box scoring function.
  - 4.1. Computing Retrieval Likelihood
  - 4.2. Computing LM likelihood
    - We use the LM as a scoring function to measure how much each document could improve the LM perplexity. 
    - Specifically, we first compute PLM(y | d, x), the LM probability of the ground truth output y given the input context x and a document d. 
    - The higher the probability, the better the document di is at improving the LMâ€™s perplexity. 
  - 4.3. Loss Function ( KL divergence )
  - 4.4. Asynchronous Update of the Datastore Index
- Wed, 3 May 2023 [Improving Contrastive Learning of Sentence Embeddings from AI Feedback](https://arxiv.org/abs/2305.01918)
  - we first mask some words of the original sentence with different mask rates using the <mask> token, in order to delete some information in the original sentence.
  - Then we write a task description prompt to steer GPT-3 to generate new sentences based on masked sentences. 
  - We write a task description prompt to steer GPT-3 to generate a similarity score between 0 and 1 for each sample pair generated in step 1
  - å±…ç„¶æ•ˆæœæ¯” SimCSE å¥½ï¼Œæ˜¯ SimCSE æ•ˆæœå¤ªå·®äº†å—
- Sat, 21 Oct 2023 [Beyond Yes and No: Improving Zero-Shot LLM Rankers via Scoring Fine-Grained Relevance Labels](https://arxiv.org/abs/2310.14122)
  - However, the lack of intermediate relevance label options may cause the LLM to provide noisy or biased answers for documents that are partially relevant to the query. 
  - We propose to incorporate fine-grained relevance labels into the prompt for LLM rankers, 
  - enabling them to better differentiate among documents with different levels of relevance to the query and thus derive a more accurate ranking
- Fri, 29 Mar 2024 [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327)
  - LLM-based Positive and Negative Mining
    - we use an existing embedding model1 to retrieve top ğ‘ neighbors ğ‘ƒ from the corpus given a generated query ğ‘. 
    - We then employ the same LLM used for the query generation to rank these retrieved passages based on their relevance to the query
      - query likelihoodï¼Œ relevance classification
    - we create the FRet dataset, comprised of 6.6M examples, each containing a task, a query, a positive passage, and a negative passage.
    - we find that using the most relevant passage chosen by an LLM is always better than using the original passage as positive.
### reasoning-intensive
- Tue, 29 Apr 2025 [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
  - Varied-length Synthetic Query and Positive Document Generation
  - Reasoning-intensive Document-to-query Generation
  
# Knowledge distillation
éšç€å¼€æºçš„æ¨¡å‹è¶Šæ¥è¶Šå¤šï¼ŒçŸ¥è¯†è’¸é¦è¶Šæ¥è¶Šæˆä¸ºé«˜æ•ˆçš„è®­ç»ƒæ‰‹æ®µ
- Tue, 14 Dec 2021 [GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval](https://arxiv.org/abs/2112.07577)
  - In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. 
  - As we show in Appendix E, just using these unsupervised techniques is not sufficient and the resulting models perform poorly.
  - Query Generation via T5 (DocT5Query)
  - Negative Mining via Dense Retrieval (msmarco-distilbert-base-v3 and msmarco-MiniLML-6-v3)
  - Pseudo Labeling via Cross-Encoder  (ms-marco-MiniLM-L6-v2 cross-encoder)
    - Previous work has shown that cross-encoders achieve much higher performances 
    - and are less prone to domain shifts
  - MultipleNegativesRanking(MNRL) loss
- Mon, 19 Aug 2024 [Improving embedding with contrastive fine-tuning on small datasets with expert-augmented scores](https://arxiv.org/abs/2408.11868)
  - utilizes soft labels derived from expert-augmented scores
- Thu, 26 Dec 2024 [Jasper and Stella: distillation of SOTA embedding models](https://arxiv.org/abs/2412.19048)
  - Stage 1&2: Distillation from Multiple Teachers
    - Lcosine
    - Lsim
    - Lresim teacher models to automatically generate soft labels for all text pairs
    - The biggest advantage of distillation vectors is that we do not need any supervised data
    - In stage 1, stage 2 and stage 3, we use fineweb-edu as our main text training dataset
  - Stage 3: Dimension Reduction
    - Matryoshka Embedding
  - Stage 4: Unlock Multimodal Potential
- Thu, 7 Nov 2024 [Best Practices for Distilling Large Language Models into BERT for Web Search Ranking](https://arxiv.org/abs/2411.04539)
  -  Knowledge Distillation with Rank Loss
- Wed, 26 Mar 2025 [Dewey Long Context Embedding Model: A Technical Report](https://arxiv.org/abs/2503.20376)
  - Architecture
    - modernbert-large
  - Training Recipe
    - Chunk-Alignment Training
      - Our model can generate three types of embeddings:Late Chunking çš„å«é‡‘é‡åœ¨ä¸æ–­å‡é«˜
        - CLS embedding
        - Chunk embeddings
        - Mean embedding
    - Knowledge distillation
      - We use Linq-Embed-Mistral(Kim et al., 2024) as our teacher model.
      - We get unsupervised texts from Infinity-Instructand fineweb-edu. 
      - ç›´æ¥ç”¨ unsupervised texts åš Knowledge distillation å•Š
      - We take two strategies to split text to chunks:
        - Split Text by Word 30% probability
        - RecursiveCharacterTextSplitter in langchain 70% probability

# Hard Negative Mining
Contrastive Pre-training ä½¿ç”¨å¤§ batchsize in-batch negativesï¼ŒSupervised Fine-tuning ä½¿ç”¨å¦ä¸€ä¸ªå¼ºæ¨¡å‹ç¦»çº¿æ‰¾å¥½è´Ÿæ ·æœ¬
- Wed, 1 Jul 2020 [Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval](https://arxiv.org/abs/2007.00808)
  - Inefficacy of Local In-Batch Negatives
  - Approximate nearest neighbor Negative Contrastive Learning (ANCE)
    - Asynchronous Index Refresh
    - åœ¨çº¿æœå…¨å±€ Hard Negative
    - ç°åœ¨ Supervised Fine-tuning é˜¶æ®µéƒ½ä½¿ç”¨å¦å¤–ä¸€ä¸ªå¼ºæ¨¡å‹ç¦»çº¿ Hard Negative Mining
- Fri, 16 Oct 2020 [RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2010.08191)
  - Cross-batch Negatives
  - Denoised Hard Negatives
  - Data Augmentation
- Thu, 14 Sep 2023 [C-Pack: Packed Resources For General Chinese Embeddings](https://arxiv.org/abs/2309.07597)
  - Contrastive Pre-training
    - we purely rely on in-batch negative samples [25] and resort to a big batch size (as large as 19,200) to improve the discriminativeness of the embedding.
- Sun, 4 Feb 2024 [ä¸ºRAGè€Œç”Ÿ-BCE embeddingæŠ€æœ¯æŠ¥å‘Š](https://zhuanlan.zhihu.com/p/681370855)
  - éš¾è´Ÿæ ·ä¾‹æŒ–æ˜ï¼Ÿ
    - æˆ‘ä»¬åœ¨è®­ç»ƒEmbeddingæ¨¡å‹æ—¶å‘ç°ï¼Œè¿‡éš¾çš„è´Ÿæ ·æœ¬å¯¹æ¨¡å‹è®­ç»ƒæœ‰æŸå®³ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šä½¿æ¨¡å‹â€œå›°æƒ‘â€ï¼Œ[å½±å“æ¨¡å‹æœ€ç»ˆæ€§èƒ½](https://kexue.fm/archives/8847#%E9%9A%BE%E6%90%9E%E7%9A%84%E9%98%88%E5%80%BC)ã€‚
    - åœ¨å¤§é‡çš„è¯­æ–™åº“ä¸­ï¼Œè„±ç¦»äººå·¥æ ¡éªŒçš„è‡ªåŠ¨åŒ–éš¾è´Ÿæ ·ä¾‹æŒ–æ˜ï¼Œéš¾å…ä¼šâ€œæŒ–åˆ°æ­£ä¾‹â€ã€‚
- Wed, 8 May 2024 [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374)
  - Supervised Fine-tuning
    - Tunable Hard Negative Mining
- Sat, 11 May 2024 [Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](https://arxiv.org/abs/2405.06932)
  - For each retrieval task, we use piccolo-base-zh [12] to conduct negative sample mining. 
  - We randomly select 15 samples from the mining negatives of rank 50 - 100 as the final hard negative samples. 
  - We avoid using higher-rank negative samples as their inclusion typically leads to a decline in performance. 
  - This is caused by a variety of reasons, such as inaccurate dataset annotation.
- Mon, 22 Jul 2024 [NV-Retriever: Improving text embedding models with effective hard-negative mining](https://arxiv.org/abs/2407.15831)
  - hard-negative mining
- Fri, 28 Oct 2024 [SFR-Embedding-Mistral: Enhance Text Retrieval with Transfer Learning](https://www.salesforce.com/blog/sfr-embedding/)
  - Impact of Hard Negatives
    - Strategy to Eliminate False Negatives
      - The results indicate that the range from 30 to 100 yields improved performance. 
      - This implies that the top-ranked documents (0-100) may include some false negatives, 
      - while those ranked lower (50-100) lack sufficient challenge.
    - Number of Hard Negatives
      - Nevertheless, our findings suggest that the training process remains relatively stable regardless of the number of hard negatives utilized.
    - Impact of Batch Size
      -  However, enlarging the batch size from 2048 to 8192 does not result in any significant change in performance.
    - Teacher models for hard negative mining
      - in general, more powerful models can yield more effective hard negatives (SFR-Embedding-Mistral > E5-Mistral > BGE-base). 
      - In the future, it will be intriguing to explore the impact of multi-round training on two fronts

# Loss
- Tue, 27 Aug 2019 [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
  - Objective Function
    - Classification Objective Function o = softmax(Wt(u, v, |u âˆ’ v|))
    - Regression Objective Function cosine-sim(u, v)
    - Triplet Objective Function max(||sa âˆ’ sp|| âˆ’ ||sa âˆ’ sn|| + s, 0)
- Tue, 25 Feb 2020 [Circle Loss: A Unified Perspective of Pair Similarity Optimization](https://arxiv.org/abs/2002.10857)
- Fri, 22 Sep 2023 [AnglE-optimized Text Embeddings](https://arxiv.org/abs/2309.12871)
  - ANGLE OBJECTIVE 
- Mon, 27 Mar 2023 [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343)
  - The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. 
- Sat, 11 May 2024 [Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training](https://arxiv.org/abs/2405.06932)
  - Multi-task Hybrid Loss
    - Retrieval and Reranking Lossï¼Œuse the standard InfoNCE loss with in-batch negative
    - STS and PairClassification Lossï¼Œcosent loss function
    - Classification and Clustering Lossï¼ŒSFR embedding method
- Wed, 11 Jun 2025 [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
  - we propose an auxiliary loss that reduces the variance of negative-pair similarities in mini-batch settings. 
  - Empirical results show that incorporating the proposed loss improves performance in small-batch settings.

# PEFT
- Wed, 24 Aug 2022 [DPTDR: Deep Prompt Tuning for Dense Passage Retrieval](https://arxiv.org/abs/2208.11503)
  - ä½¿ç”¨ Deep Prompt Tuning èƒ½å¾—åˆ°æœ‰ç«äº‰åŠ›çš„æ¨¡å‹å—
- Mon, 16 Sep 2024 [jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173)
  - ä½¿ç”¨ lora å‘¢ 
- Fri, 28 Oct 2024 [SFR-Embedding-Mistral: Enhance Text Retrieval with Transfer Learning](https://www.salesforce.com/blog/sfr-embedding/)
  - The SFR-Embedding-Mistral marks a significant advancement in text-embedding models, building upon the solid foundations of E5-mistral-7b-instruct and Mistral-7B-v0.1
  - LoRA adapters with rank r=8 are added to all linear layers, resulting in 21M trainable parameters. 