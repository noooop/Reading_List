

# Positional Encoding
- Mon, 12 Jun 2017 [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - In this work, we use sine and cosine functions of different frequencies
- Thu, 11 Oct 2018 [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
  - Learnable Token Embeddings + Segment Embeddings + Position Embeddings
- Sun, 28 Jun 2020 [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)
  - 位置嵌入应该在attention里面计算，而不是在加在词向量里
- Tue, 20 Apr 2021 [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
  - 大名鼎鼎的 Rotary Position Embedding